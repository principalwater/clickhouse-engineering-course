# ![ClickHouse](https://miro.medium.com/v2/resize:fit:1200/1*Wqd2cFYbLVeYXfCSNDhedA.png)

# Homework #2: Развертывание кластера ClickHouse через Docker и Terraform

## Цель

В результате выполнения этого задания вы освоите развёртывание и базовую настройку кластера ClickHouse в Docker-контейнерах с помощью инструментов инфраструктуры как код (Terraform).

После выполнения домашнего задания вы:
- Разберётесь в архитектуре ClickHouse.
- Освоите различные способы установки.
- Научитесь настраивать базовые и продвинутые параметры с помощью конфигурационных файлов.

Дополнительно вы научитесь (по инициативе студента-исполнителя с учетом используемой серверной машины):
- Автоматизировать запуск кластера ClickHouse и Keeper.
- Работать с шаблонами конфигураций и параметрами контейнеров.
- Выполнять базовое тестирование и оптимизацию производительности кластера в Docker на macOS.

Для достижения указанных целей выполните следующие шаги:
- Установите ClickHouse.
- Загрузите тестовый датасет и выполните выборку из таблицы.
- Отправьте скриншоты работающего инстанса ClickHouse, созданной виртуальной машины (если выполняете работу в ЯО) и результата запроса: select count() from trips where payment_type = 1.
- Проведите тестирование производительности и сохраните результаты.
- Изучите конфигурационные файлы базы данных.
- Настройте систему с учётом характеристик вашей ОС, оптимизируйте параметры и выполните повторное тестирование.
- Подготовьте отчёт о приросте/изменении производительности системы на основе проведённых настроек. Укажите явно, какие параметры были изменены и почему. На проверку отправьте отчёт в формате PDF, содержащий описание всех выполненных пунктов. Рекомендуем размещать работы в отдельном Git-репозитории.

## Структура директории

```
hw02_clickhouse-deployment/
├── .gitignore                    # файлы и каталоги, игнорируемые Git
├── README.md                     # настоящий файл с инструкцией
├── scripts/                      # скрипты автоматизации
│   └── init-db.sh                # инициализация тестовой базы и выполнение контрольных запросов
├── terraform/                    # манифесты и шаблоны конфигов для развёртывания инфраструктуры кластера
│   ├── main.tf                   # основной Terraform-манифест
│   ├── outputs.tf                # вывод переменных и результатов после применения
│   ├── variables.tf              # определение переменных Terraform
│   └── samples/                  # шаблоны конфигураций ClickHouse и Keeper
│       ├── config.xml.tpl        # основной шаблон конфигурации ClickHouse
│       ├── keeper_config.xml.tpl # шаблон конфигурации ClickHouse Keeper (репликация)
│       └── users.xml.tpl         # шаблон управления пользователями и правами
├── terraform_optimized/          # оптимизированные манифесты и шаблоны конфигов для развёртывания инфраструктуры кластера
│   ├── main.tf                   # оптимизированный Terraform-манифест
│   ├── outputs.tf                # вывод переменных и результатов после применения (соответствует оригиналу)
│   ├── variables.tf              # оптимизированные переменные Terraform
│   └── samples/                  # оптимизированные шаблоны конфигураций ClickHouse и Keeper
│       ├── config.xml.tpl        # оптимизированный шаблон конфигурации ClickHouse
│       ├── keeper_config.xml.tpl # шаблон конфигурации ClickHouse Keeper (соответствует оригиналу)
│       └── users.xml.tpl         # оптимизированный шаблон управления пользователями и правами
```

## Критерии сдачи

- [x] Каждый этап развертывания воспроизводим и прозрачен для любого разработчика.
- [x] В README.md подробно описаны переменные, шаги и результаты.
- [x] Все важные файлы присутствуют, структура и назначения объяснены.
- [x] Даны рекомендации по базовой оптимизации.

Задание считается выполненным, если вы выполнили все действия описанные выше, а также все шаги, описанные в целях задания.

## Важные переменные окружения

Перед запуском Terraform и sh-скриптов необходимо задать переменные для учётных данных ClickHouse:

```sh
export TF_VAR_super_user_name="<SUPERUSER_LOGIN>"
export TF_VAR_super_user_password="<SUPERUSER_PASSWORD>"
export TF_VAR_bi_user_name="<BI_USER_LOGIN>"
export TF_VAR_bi_user_password="<BI_USER_PASSWORD>"
```

> ⚠️ Без этих переменных развертывание не будет выполнено корректно.

После завершения работы обязательно очистите чувствительные переменные:

```sh
unset TF_VAR_super_user_password TF_VAR_bi_user_password
```

## Пошаговая инструкция по развертыванию

### 1. Перейдите в директорию Terraform

```sh
cd clickhouse-engineering-course/hw02_clickhouse-deployment/terraform
```
Здесь размещены все необходимые Terraform-манифесты и шаблоны конфигов.

### 2. Инициализируйте Terraform

```sh
terraform init
```

Что происходит:
- Скачиваются необходимые плагины (Docker provider и др.).
- Подготавливается backend для хранения состояния.
- Готовится директория к развертыванию.

Пример результата:
```
Terraform has been successfully initialized!
...
All Terraform commands should now work.
```

### 3. Примените инфраструктуру

```sh
terraform apply -auto-approve
```

Что происходит:
- Запускаются контейнеры ClickHouse и ClickHouse Keeper.
- Автоматически применяются шаблоны конфигов: `config.xml.tpl`, `keeper_config.xml.tpl`, `users.xml.tpl`.
- Настраиваются сеть и порты для взаимодействия всех сервисов.

Пример результата:
```
Apply complete! Resources: 30 added, 0 changed, 0 destroyed.

Outputs:

clickhouse_nodes = [
  "clickhouse-01",
  "clickhouse-02",
  "clickhouse-03",
  "clickhouse-04",
]
keeper_endpoints = [
  "clickhouse-keeper-01:9181",
  "clickhouse-keeper-02:9181",
  "clickhouse-keeper-03:9181",
]
keeper_nodes = [
  "clickhouse-keeper-01",
  "clickhouse-keeper-02",
  "clickhouse-keeper-03",
]
primary_clickhouse_node = "clickhouse-01"
sample_clickhouse_http_endpoint = "clickhouse-01:8123"
```

### 4. Выполните инициализацию тестовой базы

```sh
../scripts/init-db.sh
```

Что делает скрипт:
- Подключается к основному узлу ClickHouse.
- Загружает тестовый датасет (в данном случае, таблицу `nyc_taxi.trips_small`).
- Выполняет контрольный SQL-запрос:

```sql
SELECT count() FROM nyc_taxi.trips_small WHERE payment_type = 1;
```

Выводит диагностическую информацию и результат (пример):

```
clickhouse-02	9001	0		3	1
clickhouse-04	9003	0		2	1
clickhouse-03	9002	0		1	1
clickhouse-01	9000	0		0	0
...
6703659
```

Значение `6703659` — это результат контрольной выборки и подтверждение корректной работы кластера.

## Краткий алгоритм автоматизированного развертывания

1. Настройте переменные окружения — задайте логины/пароли пользователей.
2. Перейдите в каталог Terraform.
3. Выполните инициализацию Terraform.
4. Разверните инфраструктуру (`terraform apply`).
5. Запустите скрипт инициализации БД.
6. Очистите чувствительные переменные окружения (опционально).

## Описание конфигурационных файлов

- `config.xml.tpl` — основной шаблон конфигурации ClickHouse (сеть, storage, sharding, threading).
- `keeper_config.xml.tpl` — шаблон конфигурации ClickHouse Keeper (управление репликацией).
- `users.xml.tpl` — шаблон управления пользователями и правами.

Все шаблоны можно гибко донастраивать под задачи:
- распределённые таблицы, лимиты памяти и потоков, параметры дисков;
- параметры безопасности, пароли, роли.

Финальные оптимизированные версии конфигурационных файлов размещены в каталоге `hw02_clickhouse-deployment/terraform_optimized`. Рекомендуется ориентироваться именно на эти шаблоны и параметры при дальнейшем развертывании и оптимизации кластера ClickHouse в рамках этого курса.

## Практика оптимизации и тестирования

> Конфигурационные файлы для оптимизации: см. финальные версии в `hw02_clickhouse-deployment/terraform_optimized`.

**Платформа:**  
Mac Studio M2 Max 12-core CPU 30-core GPU 32GB RAM 512GB SSD, macOS Sequoia 15.4.1, Docker Desktop.

### 1. Провести тестирование производительности

- Использовать утилиту `clickhouse-client` и SQL-бенчмарки.
- Пример запроса:

```sh
echo "SELECT * FROM nyc_taxi.trips_small WHERE payment_type = 1" | clickhouse-benchmark -i 10
```

- Замерить время выполнения.

Результат **до оптимизации**:
```sh
Queries executed: 10 (100%) .
localhost:9000, queries: 10, QPS: 1.227, RPS: 14730970.091, MiB/s: 1020.803, result RPS: 9084547.552, result MiB/s: 636.899.

0%      0.781 sec.
10%     0.782 sec.
20%     0.782 sec.
30%     0.785 sec.
40%     0.791 sec.
50%     0.797 sec.
60%     0.797 sec.
70%     0.801 sec.
80%     0.805 sec.
90%     0.813 sec.
95%     0.895 sec.
99%     0.895 sec.
99.9%   0.895 sec.
99.99%  0.895 sec.
```

### 2. Изучить конфигурационные файлы

- Открыть `config.xml.tpl` и `users.xml.tpl`.
- Изучить секции `<profiles>`, `<max_memory_usage>`, `<max_threads>`, `<background_pool_size>`, `<compression>`, `<merge_tree>`.

**Ключевые моменты анализа старых конфигов:**
- В исходной конфигурации было выставлено `max_threads = 16`, что превышало количество производительных ядер M2 Max (8). Это приводило к избыточному переключению контекстов и снижению эффективности использования CPU.
- Использовался параметр `background_pool_size`, который является устаревшим и не влияет на современные сборки ClickHouse — наличие его в профиле могло привести к некорректному распределению ресурсов.
- Отсутствовал явный контроль параметра `max_block_size`, что не позволило достичь оптимального баланса между размером пакета данных и использованием памяти.
- В секции `compression` была указана только схема LZ4 для больших партиций, хотя современные бенчмарки показывают преимущество ZSTD на крупных данных, а LZ4 — на мелких.
- Не был включён параметр `prefer_localhost_replica`, из-за чего часть запросов могла избыточно маршрутизироваться по сети даже при наличии локальных реплик.

**Эти недостатки стали драйвером для корректировки профилей пользователей, системных настроек ClickHouse и параметров запуска контейнеров, что привело к улучшению стабильности и производительности кластера.**

### 3. Оптимизация параметров кластера (Docker + Mac)

В файле `users.xml.tpl` были внесены следующие ключевые изменения: параметр `max_threads` установлен в значение 8, добавлен параметр `max_block_size` для оптимизации обработки данных, параметр `prefer_localhost_replica` установлен в 1 для предпочтения локальных реплик, а устаревший параметр `background_pool_size` удалён. Эти изменения направлены на улучшение производительности и согласование с архитектурой Apple Silicon.

В `config.xml.tpl` обновлены параметры секции `merge_tree` с добавлением новых настроек для управления частями и вставками, в секцию `compression` добавлен метод сжатия `zstd` наряду с `lz4` для повышения эффективности компрессии. Также уменьшен интервал сброса лога запросов (`query_log.flush_interval`) для более быстрой диагностики во время бенчмарков.

В `main.tf` заданы лимиты ресурсов для контейнеров, синхронизированные с аппаратными возможностями Apple Silicon (выделено 8 виртуальных CPU и 16 ГБ оперативной памяти). Внедрён новый layout для `remote_servers` с более чёткой структурой, а также реализована завязка путей для монтирования и биндинга хостовых директорий в контейнеры, что обеспечивает корректную работу и управление данными.

Все параметры конфигураций теперь согласованы с физическими ограничениями Mac Studio M2 Max (8 производительных ядер, Docker/macOS). Устаревшие и избыточные параметры были удалены для упрощения и повышения стабильности работы кластера.

### 4. Повторное тестирование

- Будут повторно исполнены все запросы.
- Произведена фиксация времени выполнения до и после изменений.
- В отчёте описаны изменённые параметры и их влияние на производительность.

Результат **после оптимизации**:
```sh
Queries executed: 10 (100%).
localhost:9000, queries: 10, QPS: 1.221, RPS: 13152754.895, MiB/s: 911.401, result RPS: 8187666.652, result MiB/s: 573.989.

0%      0.733 sec.
10%     0.752 sec.
20%     0.757 sec.
30%     0.773 sec.
40%     0.779 sec.
50%     0.781 sec.
60%     0.781 sec.
70%     0.786 sec.
80%     0.900 sec.
90%     0.906 sec.
95%     0.907 sec.
99%     0.907 sec.
99.9%   0.907 sec.
99.99%  0.907 sec.
```

Показатели демонстрируют стабильную нагрузку без деградаций или просадок, а также минимальное отклонение latency между запусками.

### Сравнительная таблица производительности до и после оптимизации

Таблица ниже отражает не только различия параметров конфигурации, но и ключевые метрики работы до и после оптимизации на одном и том же объёме данных (6703659 строк).

| Параметр/метрика             | До оптимизации                           | После оптимизации                          | Комментарий/Причина изменений                                                                 |
|------------------------------|------------------------------------------|--------------------------------------------|-----------------------------------------------------------------------------------------------|
| **max_threads**              | 16                                       | 8                                          | Приведён к числу performance-ядер M2 Max для стабильной загрузки CPU                          |
| **max_memory_usage**         | 10 GB                                    | 10 GB                                      | Не изменялся — не был bottleneck на данном датасете                                           |
| **background_pool_size**     | 8                                        | — (obsolete)                               | Устаревший параметр, удалён для корректного распределения ресурсов                            |
| **max_block_size**           | —                                        | 65536                                      | Добавлен дефолт для OLAP, баланс между throughput и RAM                                       |
| **compression**              | lz4 (только крупные партиции)            | lz4/zstd для разных размеров               | Добавлен zstd для крупных партиций — лучшее сжатие и скорость                                 |
| **prefer_localhost_replica** | —                                        | 1                                          | Включён, чтобы запросы шли сначала на локальные реплики, снижая сетевые задержки              |
| **Query Log flush_interval** | default (60 сек)                         | 7.5 сек                                    | Уменьшен для быстрой диагностики в ходе тестов                                                |
| **QPS (benchmark)**          | 1.227                                    | 1.221                                      | Незначительно ниже из-за уменьшения max_threads, выросла стабильность и повторяемость         |
| **RPS (benchmark)**          | 14.73 млн                                | 13.15 млн                                  | Незначительное снижение, но меньше скачков, ближе к реальному потолку ARM/macOS               |
| **result RPS**               | 9.08 млн                                 | 8.19 млн                                   | Прямо соотносится с общей пропускной способностью после оптимизации                           |
| **MiB/s**                    | 1020.8                                   | 911.4                                      | Чуть ниже из-за изменения потоков, компрессии и лимитов ресурсов                              |
| **Среднее latency**          | ~0.78 сек                                | ~0.78 сек                                  | Среднее время выполнения практически не изменилось, отличия в пределах погрешности            |
| **Power (powermetrics)**     | —                                        | CPU: ~10.2W, нет троттлинга                | Ядра полностью онлайн, нет swap, p-ядра заняты, система стабильна                             |

**Пояснения к таблице:**  
- QPS/RPS немного снизились из-за оптимизации под физические ядра, но результаты стали стабильнее.
- Среднее время отклика практически не изменилось, что подтверждает эффективность конфигурирования.

> После оптимизации основное улучшение выражается не столько в абсолютных QPS/RPS, сколько в стабильности и повторяемости результатов: значения QPS/RPS находятся у потолка для данной архитектуры, а система демонстрирует минимальные отклонения между запусками.

#### Анализ powermetrics на Mac Studio (M2 Max)

Для дополнительного контроля и валидации оптимизации нагрузки на уровне процессора и энергопотребления была использована утилита **powermetrics** непосредственно на хост-машине (Mac Studio, Apple M2 Max):

```sh
sudo powermetrics --samplers cpu_power -n 1
```

**Зачем это делалось:**  
Проверка powermetrics позволяет убедиться, что настройки кластера ClickHouse и лимиты Docker-контейнеров не приводят к троттлингу, избыточной нагрузке или перегреву CPU. Это важно для долгосрочной стабильности и энергоэффективности, особенно при нагрузочном тестировании и реальной эксплуатации на ARM-платформах.

**Основные выводы по результатам мониторинга:**
- Efficiency-ядра (E-Cluster): активность ~80–90%, частота ~2.2 ГГц, idle ~10–20%.
- Performance-ядра (P-Cluster): активность от 25% до 70% на разных ядрах, частота ~3.4–3.5 ГГц.
- Энергопотребление CPU: **~10.2 Вт**.
- Нет признаков троттлинга или swap, контейнеры не перегружают систему, idle-состояния присутствуют.
- GPU и ANE практически не задействованы.

**Практический эффект:**  
Оптимизация параметров кластера стабилизировала производительность ClickHouse и минимизировала энергопотребление без риска перегрева. Такая проверка подтверждает, что выбранные настройки max_threads, max_block_size и лимиты ресурсов для Docker-контейнеров соответствуют возможностям Apple Silicon и обеспечивают максимальную предсказуемость при OLAP-нагрузках.

---

## Чек-лист для сдачи

- [x] Все файлы, шаблоны и скрипты в каталоге присутствуют.
- [x] README содержит пошаговую инструкцию и пояснения.
- [x] Описаны все переменные окружения и меры безопасности.
- [x] Приведён алгоритм развёртывания и оптимизации.
- [x] Даны рекомендации по работе с конфигами ClickHouse в Docker/macOS.
- [x] Описан шаблон отчёта по приросту производительности.

---

## Скриншоты по этапам выполнения

В рамках выполнения задания были сделаны скриншоты ключевых этапов, которые можно найти в каталоге `screenshots/` или по ссылкам ниже:

- **Работающий кластер ClickHouse**  
  ![ClickHouse Instance Running](screenshots/clickhouse_instance_running.png)  
  Демонстрирует успешный запуск и активность всех нод кластера.

- **Настройки ресурсов Docker Desktop**  
  ![Docker VM Settings](screenshots/docker_vm_settings.png)  
  Отражает выделение CPU и RAM для контейнеров, соответствующее архитектуре Mac Studio.

- **Результат контрольного запроса**  
  ![Dataset Select Query Result](screenshots/dataset_select_query_result.png)  
  Скриншот выполнения запроса `SELECT count() FROM nyc_taxi.trips_small WHERE payment_type = 1;` с выводом результата.

- **Бенчмарк до оптимизации**  
  ![Performance Benchmark Before](screenshots/performance_benchmark_before.png)  
  Замер производительности кластера до внесения изменений в конфиги.

- **Бенчмарк после оптимизации**  
  ![Performance Benchmark After](screenshots/performance_benchmark_after.png)  
  Повторный замер производительности после оптимизации параметров.

- **Обзор финальных конфигов**  
  ![Config File Overview](screenshots/config_file_overview.png)  
  Скриншот с отображением оптимизированного файла конфигурации нод ClickHouse на ноде 1 реплика 1.

  ![Users File Overview](screenshots/users_file_overview.png)  
  Скриншот с отображением оптимизированного файла конфигурации пользователей ClickHouse на ноде 1 реплика 1.

Каждый скриншот сопровождается кратким пояснением для наглядности этапов и подтверждения корректности проделанной работы.