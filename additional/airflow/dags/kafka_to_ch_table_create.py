from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
import json
import os
import sys
from dotenv import load_dotenv

# Load environment variables
dotenv_path = os.path.join(os.path.dirname(__file__), '../../infra/env/clickhouse.env')
load_dotenv(dotenv_path=dotenv_path)

# –ü–æ–ª—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –∏–∑ Kafka –≤ ClickHouse
# –≠—Ç–∏ –ø–æ–ª—è –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω—ã –∏–∑ String –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–∏–ø—ã
# –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ ClickHouse
DATE_FIELDS = ['date', 'event_date', 'created_date', 'updated_date']  # String -> Date
DATETIME_FIELDS = ['timestamp', 'created_at', 'updated_at', 'event_time']  # String -> DateTime

def fix_projection_order_by(order_by_clause):
    """
    –ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç ORDER BY –¥–ª—è ClickHouse projection.
    –í projection ClickHouse –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —è–≤–Ω—ã–µ ASC/DESC –≤ —Ç–æ–º –∂–µ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–µ.
    –î–ª—è –ø—Ä–æ–µ–∫—Ü–∏–π —Å —Å–º–µ—à–∞–Ω–Ω–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–æ–π –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—ã–µ –ø–æ–ª—è.
    """
    if not order_by_clause:
        return order_by_clause
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ DESC –≤ —Å—Ç—Ä–æ–∫–µ
    if 'DESC' in order_by_clause.upper():
        # –î–ª—è projection —Å DESC –ª—É—á—à–µ –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è –±–µ–∑ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        # –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ–ª—è —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º
        fields = [field.strip() for field in order_by_clause.split(',')]
        clean_fields = []
        
        for field in fields:
            # –£–±–∏—Ä–∞–µ–º ASC/DESC –∏–∑ –ø–æ–ª—è, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∏–º—è
            clean_field = field.replace(' DESC', '').replace(' ASC', '').strip()
            clean_fields.append(clean_field)
        
        result = ', '.join(clean_fields)
        if result != order_by_clause:
            print(f"üîß –£–ø—Ä–æ—â–µ–Ω ORDER BY –¥–ª—è projection (—É–±—Ä–∞–Ω—ã ASC/DESC): '{order_by_clause}' ‚Üí '{result}'")
        return result
    
    return order_by_clause

def check_existing_tables(client, target_table_name, database_name, kafka_database):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–∞–±–ª–∏—Ü"""
    existing_tables = []
    
    # –°–ø–∏—Å–æ–∫ —Ç–∞–±–ª–∏—Ü –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    tables_to_check = [
        (kafka_database, f"{target_table_name}_kafka"),
        (database_name, f"{target_table_name}_local"),
        (database_name, target_table_name),
        (database_name, f"{target_table_name}_mv")
    ]
    
    for db, table in tables_to_check:
        try:
            result = client.query(f"EXISTS {db}.{table}")
            if result.result_rows and result.result_rows[0][0] == 1:
                existing_tables.append(f"{db}.{table}")
        except Exception:
            # –¢–∞–±–ª–∏—Ü–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞
            pass
    
    return existing_tables

def get_clickhouse_config():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ClickHouse –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
    host = os.getenv("CLICKHOUSE_HOST")
    port = os.getenv("CLICKHOUSE_PORT")
    username = os.getenv("CH_USER")
    password = os.getenv("CH_PASSWORD")
    
    missing_vars = []
    if not host:
        missing_vars.append("CLICKHOUSE_HOST")
    if not port:
        missing_vars.append("CLICKHOUSE_PORT")
    if not username:
        missing_vars.append("CH_USER")
    if not password:
        missing_vars.append("CH_PASSWORD")
    
    if missing_vars:
        raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è ClickHouse: {', '.join(missing_vars)}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª infra/env/clickhouse.env")
    
    return {
        'host': host,
        'port': int(port),
        'username': username,
        'password': password,
        'secure': False
    }

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ DAG
default_args = {
    'owner': 'energy-hub',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# –°–æ–∑–¥–∞–Ω–∏–µ DAG –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü –∏–∑ Kafka –≤ ClickHouse
kafka_to_ch_dag = DAG(
    'kafka_to_ch_table_create',
    default_args=default_args,
    description='–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü ClickHouse –∏–∑ Kafka —Ç–æ–ø–∏–∫–æ–≤ (Kafka ‚Üí Materialized View ‚Üí Distributed)',
    schedule=None,  # –†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫
    catchup=False,
    tags=['clickhouse', 'kafka', 'tables', 'kafka-to-clickhouse'],
)

def get_table_config(**context):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü—ã –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ DAG"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ DAG
        dag_run = context['dag_run']
        conf = dag_run.conf if dag_run else {}
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è —Å—Ö–µ–º—ã Kafka ‚Üí Materialized View ‚Üí Distributed –≤ DWH –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ
        default_config = {
            'kafka_topic': 'covid_new_cases_1min',
            'target_table_name': 'covid_new_cases',
            'dwh_layer': 'raw',  # raw, ods, dds, cdm
            'kafka_database': 'otus_kafka',  # –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Kafka —Ç–∞–±–ª–∏—Ü
            'sort_key': 'date, location_key',
            'partition_key': 'toYYYYMM(date)',
            'shard_key': 'xxHash64(location_key)',
            'cluster_name': 'dwh_test',
            'kafka_broker': 'kafka:9092',
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            'create_projection': True,  # —Å–æ–∑–¥–∞–≤–∞—Ç—å –ª–∏ –ø—Ä–æ–µ–∫—Ü–∏—é –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            'projection_order_by': None,  # –µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è sort_key
            'create_indexes': True,  # —Å–æ–∑–¥–∞–≤–∞—Ç—å –ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã
            'index_fields': [],  # –ø–æ–ª—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—É—Å—Ç–æ
            'table_settings': {},  # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
            'skip_alter_on_error': True,  # –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å ALTER –∑–∞–ø—Ä–æ—Å—ã –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
            'recreate_tables': False,  # –ø–µ—Ä–µ—Å–æ–∑–¥–∞–≤–∞—Ç—å —Ç–∞–±–ª–∏—Ü—ã (—É–¥–∞–ª—è—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ)
            'schema': {
                'date': 'String',
                'location_key': 'String',
                'new_confirmed': 'Int32',
                'new_deceased': 'Int32',
                'new_recovered': 'Int32',
                'new_tested': 'Int32'
            }
        }
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        config = {**default_config, **conf}
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ—è DWH
        dwh_layer = config['dwh_layer']
        if dwh_layer == 'raw':
            database_name = 'raw'
        elif dwh_layer == 'ods':
            database_name = 'ods'
        elif dwh_layer == 'dds':
            database_name = 'dds'
        elif dwh_layer == 'cdm':
            database_name = 'cdm'
        else:
            database_name = 'raw'  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        config['database_name'] = database_name
        
        print(f"üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã –ø–æ —Å—Ö–µ–º–µ Kafka ‚Üí Materialized View ‚Üí Distributed:")
        print(f"   Kafka —Ç–æ–ø–∏–∫: {config['kafka_topic']}")
        print(f"   –¶–µ–ª–µ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞: {config['target_table_name']}")
        print(f"   –°–ª–æ–π DWH: {config['dwh_layer']}")
        print(f"   –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö DWH: {config['database_name']}")
        print(f"   –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö Kafka: {config['kafka_database']}")
        print(f"   –ö–ª—é—á —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏: {config['sort_key']}")
        print(f"   –ö–ª—é—á –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {config['partition_key']}")
        print(f"   –ö–ª—é—á —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏—è: {config['shard_key']}")
        print(f"   –ö–ª–∞—Å—Ç–µ—Ä: {config['cluster_name']}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ XCom –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á–∞—Ö
        context['task_instance'].xcom_push(key='table_config', value=config)
        
        return config
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        raise

def check_connections(**context):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π –∫ ClickHouse –∏ Kafka"""
    try:
        import clickhouse_connect
        from kafka import KafkaConsumer
        
        config = context['task_instance'].xcom_pull(task_ids='get_table_config', key='table_config')
        
        if not config:
            raise ValueError("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∑–∞–¥–∞—á–∞ get_table_config –≤—ã–ø–æ–ª–Ω–∏–ª–∞—Å—å —É—Å–ø–µ—à–Ω–æ.")
        
        print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π...")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ClickHouse
        ch_config = get_clickhouse_config()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ ClickHouse —á–µ—Ä–µ–∑ HTTP –ø–æ—Ä—Ç
        client = clickhouse_connect.get_client(**ch_config)
        
        result = client.query('SELECT version()')
        version = result.result_rows[0][0]
        print(f"‚úÖ ClickHouse: –≤–µ—Ä—Å–∏—è {version}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞
        cluster_result = client.query(f"SELECT name, host_name, port FROM system.clusters WHERE name = '{config['cluster_name']}'")
        if cluster_result.result_rows:
            print(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä {config['cluster_name']}: {len(cluster_result.result_rows)} —É–∑–ª–æ–≤")
        else:
            print(f"‚ö†Ô∏è –ö–ª–∞—Å—Ç–µ—Ä {config['cluster_name']} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        client.close()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Kafka
        consumer = KafkaConsumer(
            bootstrap_servers=[config['kafka_broker']],
            consumer_timeout_ms=5000
        )
        
        topics = consumer.topics()
        if config['kafka_topic'] in topics:
            print(f"‚úÖ Kafka —Ç–æ–ø–∏–∫ {config['kafka_topic']} –Ω–∞–π–¥–µ–Ω")
        else:
            print(f"‚ö†Ô∏è Kafka —Ç–æ–ø–∏–∫ {config['kafka_topic']} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        consumer.close()
        
        return "Success"
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π: {e}")
        raise

def generate_sql_script(**context):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SQL-—Å–∫—Ä–∏–ø—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    try:
        config = context['task_instance'].xcom_pull(task_ids='get_table_config', key='table_config')
        
        if not config:
            raise ValueError("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∑–∞–¥–∞—á–∞ get_table_config –≤—ã–ø–æ–ª–Ω–∏–ª–∞—Å—å —É—Å–ø–µ—à–Ω–æ.")
        
        print("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è SQL-—Å–∫—Ä–∏–ø—Ç–∞...")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å—Ö–µ–º—ã Kafka ‚Üí Materialized View ‚Üí Distributed
        kafka_topic = config['kafka_topic']
        target_table_name = config['target_table_name']
        database_name = config['database_name']
        kafka_database = config['kafka_database']
        dwh_layer = config['dwh_layer']
        sort_key = config['sort_key']
        partition_key = config['partition_key']
        shard_key = config['shard_key']
        cluster_name = config['cluster_name']
        kafka_broker = config['kafka_broker']
        schema = config['schema']
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º SQL-—Å–∫—Ä–∏–ø—Ç –ø–æ —Å—Ö–µ–º–µ Kafka ‚Üí Materialized View ‚Üí Distributed –≤ DWH –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ
        sql_script = f"""
-- =====================================================
-- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SQL-—Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã {target_table_name}
-- –°—Ö–µ–º–∞: Kafka Topic ‚Üí Kafka Table Engine ‚Üí Materialized View ‚Üí ReplicatedMergeTree/Distributed
-- –°–ª–æ–π DWH: {dwh_layer}
-- –¢–æ–ø–∏–∫: {kafka_topic}
-- –ë–∞–∑–∞ Kafka: {kafka_database}
-- –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü: {config.get('recreate_tables', False)}
-- =====================================================
"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–∞–Ω–¥—ã —É–¥–∞–ª–µ–Ω–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å —Ç–∞–±–ª–∏—Ü—ã
        if config.get('recreate_tables', False):
            sql_script += f"""

-- –£–¥–∞–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–∞–±–ª–∏—Ü –¥–ª—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è
DROP TABLE IF EXISTS {database_name}.{target_table_name}_mv ON CLUSTER {cluster_name};
DROP TABLE IF EXISTS {database_name}.{target_table_name} ON CLUSTER {cluster_name};
DROP TABLE IF EXISTS {database_name}.{target_table_name}_local ON CLUSTER {cluster_name};
DROP TABLE IF EXISTS {kafka_database}.{target_table_name}_kafka ON CLUSTER {cluster_name};

"""
        
        sql_script += f"""
-- –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è DWH –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
CREATE DATABASE IF NOT EXISTS {kafka_database} ON CLUSTER {cluster_name};
CREATE DATABASE IF NOT EXISTS raw ON CLUSTER {cluster_name};
CREATE DATABASE IF NOT EXISTS ods ON CLUSTER {cluster_name};
CREATE DATABASE IF NOT EXISTS dds ON CLUSTER {cluster_name};
CREATE DATABASE IF NOT EXISTS cdm ON CLUSTER {cluster_name};

-- 1. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Å –¥–≤–∏–∂–∫–æ–º Kafka –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–æ–ø–∏–∫–∞
CREATE TABLE IF NOT EXISTS {kafka_database}.{target_table_name}_kafka ON CLUSTER {cluster_name} (
"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—è —Å—Ö–µ–º—ã
        for field_name, field_type in schema.items():
            sql_script += f"    {field_name} {field_type},\n"
        
        sql_script = sql_script.rstrip(',\n') + f"""
) ENGINE = Kafka
SETTINGS
    kafka_broker_list = '{kafka_broker}',
    kafka_topic_list = '{kafka_topic}',
    kafka_group_name = 'clickhouse-{target_table_name}-consumer',
    kafka_format = 'JSONEachRow',
    kafka_num_consumers = 1,
    kafka_skip_broken_messages = 1000,
    kafka_row_delimiter = '\\n';

-- 2. –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω–µ—á–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã —Å –¥–≤–∏–∂–∫–æ–º —Å–µ–º–µ–π—Å—Ç–≤–∞ MergeTree (ReplicatedMergeTree)
CREATE TABLE IF NOT EXISTS {database_name}.{target_table_name}_local ON CLUSTER {cluster_name} (
"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—è —Å—Ö–µ–º—ã —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º —Ç–∏–ø–æ–≤
        for field_name, field_type in schema.items():
            if field_name in DATETIME_FIELDS:
                sql_script += f"    {field_name} DateTime,\n"
            elif field_name in DATE_FIELDS and field_type == 'String':
                sql_script += f"    {field_name} Date,\n"
            else:
                sql_script += f"    {field_name} {field_type},\n"
        
        sql_script = sql_script.rstrip(',\n') + f"""
) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{{shard}}/{database_name}/{target_table_name}_local/{{uuid}}/', '{{replica}}')
PARTITION BY {partition_key}
ORDER BY ({sort_key})
PRIMARY KEY ({sort_key});

-- –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
CREATE TABLE IF NOT EXISTS {database_name}.{target_table_name} ON CLUSTER {cluster_name} (
"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—è —Å—Ö–µ–º—ã —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º —Ç–∏–ø–æ–≤
        for field_name, field_type in schema.items():
            if field_name in DATETIME_FIELDS:
                sql_script += f"    {field_name} DateTime,\n"
            elif field_name in DATE_FIELDS and field_type == 'String':
                sql_script += f"    {field_name} Date,\n"
            else:
                sql_script += f"    {field_name} {field_type},\n"
        
        sql_script = sql_script.rstrip(',\n') + f"""
) ENGINE = Distributed('{cluster_name}', '{database_name}', '{target_table_name}_local', {shard_key});

-- Materialized View
CREATE MATERIALIZED VIEW IF NOT EXISTS {database_name}.{target_table_name}_mv ON CLUSTER {cluster_name} 
TO {database_name}.{target_table_name} AS
SELECT
"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—è —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º —Ç–∏–ø–æ–≤
        for field_name, field_type in schema.items():
            if field_name in DATETIME_FIELDS:
                sql_script += f"    parseDateTimeBestEffort({field_name}) AS {field_name},\n"
            elif field_name in DATE_FIELDS and field_type == 'String':
                sql_script += f"    parseDateTimeBestEffort({field_name}) AS {field_name},\n"
            else:
                sql_script += f"    {field_name},\n"
        
        sql_script = sql_script.rstrip(',\n') + f"""
FROM {kafka_database}.{target_table_name}_kafka;
"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ)
        if config.get('create_indexes', True) and config.get('index_fields'):
            for field in config['index_fields']:
                if field in schema:
                    sql_script += f"""
-- –ò–Ω–¥–µ–∫—Å –ø–æ –ø–æ–ª—é {field}
ALTER TABLE {database_name}.{target_table_name}_local ON CLUSTER {cluster_name} 
ADD INDEX IF NOT EXISTS idx_{field} {field} TYPE minmax GRANULARITY 4;

"""
        
        if config.get('create_projection', True):
            projection_order = config.get('projection_order_by') or sort_key
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ ORDER BY –¥–ª—è projection
            # ClickHouse —Ç—Ä–µ–±—É–µ—Ç —è–≤–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –ø–æ–ª–µ–π –ø—Ä–∏ —Å–º–µ—à–∞–Ω–Ω–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–µ
            projection_order = fix_projection_order_by(projection_order)
            sql_script += f"""
-- –ü—Ä–æ–µ–∫—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
ALTER TABLE {database_name}.{target_table_name}_local ON CLUSTER {cluster_name} 
ADD PROJECTION IF NOT EXISTS {target_table_name}_projection (
    SELECT * ORDER BY {projection_order}
);

-- –ú–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ü–∏–∏
ALTER TABLE {database_name}.{target_table_name}_local ON CLUSTER {cluster_name} 
MATERIALIZE PROJECTION {target_table_name}_projection;

"""
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º SQL-—Å–∫—Ä–∏–ø—Ç –≤ XCom
        context['task_instance'].xcom_push(key='sql_script', value=sql_script)
        
        print(f"‚úÖ SQL-—Å–∫—Ä–∏–ø—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã {target_table_name}")
        print(f"üìã –†–∞–∑–º–µ—Ä —Å–∫—Ä–∏–ø—Ç–∞: {len(sql_script)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"üìã –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤: {sql_script[:500]}")
        
        return "Success"
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SQL-—Å–∫—Ä–∏–ø—Ç–∞: {e}")
        raise

def execute_sql_script(**context):
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL-—Å–∫—Ä–∏–ø—Ç–∞"""
    try:
        import clickhouse_connect
        
        config = context['task_instance'].xcom_pull(task_ids='get_table_config', key='table_config')
        sql_script = context['task_instance'].xcom_pull(task_ids='generate_sql_script', key='sql_script')
        
        if not config:
            raise ValueError("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∑–∞–¥–∞—á–∞ get_table_config –≤—ã–ø–æ–ª–Ω–∏–ª–∞—Å—å —É—Å–ø–µ—à–Ω–æ.")
        
        if not sql_script:
            raise ValueError("SQL-—Å–∫—Ä–∏–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∑–∞–¥–∞—á–∞ generate_sql_script –≤—ã–ø–æ–ª–Ω–∏–ª–∞—Å—å —É—Å–ø–µ—à–Ω–æ.")
        
        print("üîÑ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL-—Å–∫—Ä–∏–ø—Ç–∞...")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ClickHouse
        ch_config = get_clickhouse_config()
        
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse
        client = clickhouse_connect.get_client(**ch_config)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã (–±–æ–ª–µ–µ —É–º–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ)
        # –£–±–∏—Ä–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        lines = []
        for line in sql_script.split('\n'):
            line = line.strip()
            if line and not line.startswith('--'):
                lines.append(line)
        
        # –°–æ–±–∏—Ä–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã
        queries = []
        current_query = []
        in_multiline = False
        
        for line in lines:
            if line.startswith('CREATE') or line.startswith('ALTER') or line.startswith('DROP'):
                # –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
                if current_query:
                    queries.append(' '.join(current_query))
                current_query = [line]
                in_multiline = True
            elif in_multiline:
                current_query.append(line)
                if line.endswith(';'):
                    # –ó–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è –∑–∞–ø—Ä–æ—Å
                    queries.append(' '.join(current_query))
                    current_query = []
                    in_multiline = False
            else:
                # –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
                if line.endswith(';'):
                    queries.append(line)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–ø—Ä–æ—Å, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        if current_query:
            queries.append(' '.join(current_query))
        
        print(f"üìã –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ {len(queries)} SQL-–∑–∞–ø—Ä–æ—Å–æ–≤...")
        print(f"üìã –†–∞–∑–º–µ—Ä SQL-—Å–∫—Ä–∏–ø—Ç–∞: {len(sql_script)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"üìã –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ SQL: {sql_script[:500]}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–∞–±–ª–∏—Ü –ø–µ—Ä–µ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º
        target_table_name = config['target_table_name']
        database_name = config['database_name'] 
        kafka_database = config['kafka_database']
        recreate_tables = config.get('recreate_tables', False)
        
        existing_tables = check_existing_tables(client, target_table_name, database_name, kafka_database)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤—Å–µ –ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ç–∞–±–ª–∏—Ü—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        expected_tables = [
            f"{kafka_database}.{target_table_name}_kafka",
            f"{database_name}.{target_table_name}_local", 
            f"{database_name}.{target_table_name}",
            f"{database_name}.{target_table_name}_mv"
        ]
        
        all_tables_exist = all(table in existing_tables for table in expected_tables)
        
        if not recreate_tables and existing_tables:
            print("‚ÑπÔ∏è  –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –°–£–©–ï–°–¢–í–£–Æ–©–ò–• –¢–ê–ë–õ–ò–¶–ê–•:")
            for table_info in existing_tables:
                print(f"   ‚úÖ {table_info} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            
            if all_tables_exist:
                print("üéØ –í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ç–∞–±–ª–∏—Ü—ã —É–∂–µ —Å–æ–∑–¥–∞–Ω—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL-–∑–∞–ø—Ä–æ—Å–æ–≤")
                print("‚úÖ SQL-—Å–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ: –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç")
                
                # –ó–∞–≤–µ—Ä—à–∞–µ–º —Å —É—Å–ø–µ—Ö–æ–º
                client.close()
                return {
                    'status': 'success_skip',
                    'message': '–í—Å–µ —Ç–∞–±–ª–∏—Ü—ã —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç',
                    'existing_tables': existing_tables,
                    'skipped_queries': len(queries)
                }
        
        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–ø—Ä–æ—Å–∞—Ö
        for i, query in enumerate(queries, 1):
            print(f"üìã –ó–∞–ø—Ä–æ—Å {i}: {query[:100]}...")
        
        failed_queries = []
        for i, query in enumerate(queries, 1):
            if query.strip():
                try:
                    print(f"üîÑ –ó–∞–ø—Ä–æ—Å {i}/{len(queries)}: {query[:100]}...")
                    client.command(query)
                    print(f"‚úÖ –ó–∞–ø—Ä–æ—Å {i} –≤—ã–ø–æ–ª–Ω–µ–Ω")
                except Exception as e:
                    error_str = str(e)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –æ—à–∏–±–∫–æ–π –æ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ–±—ä–µ–∫—Ç–∞—Ö
                    is_already_exists = (
                        "already exists" in error_str.lower() or 
                        "ILLEGAL_COLUMN" in error_str or  # –∏–Ω–¥–µ–∫—Å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                        "ILLEGAL_PROJECTION" in error_str or  # –ø—Ä–æ–µ–∫—Ü–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                        "583" in error_str  # –∫–æ–¥ –æ—à–∏–±–∫–∏ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –ø—Ä–æ–µ–∫—Ü–∏–∏
                    )
                    
                    if is_already_exists and not recreate_tables:
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ —É—Å–ø–µ—à–Ω—ã–π —Å–ª—É—á–∞–π –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤
                        print(f"‚ÑπÔ∏è  –ó–∞–ø—Ä–æ—Å {i}: –æ–±—ä–µ–∫—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                        if "projection" in query.lower():
                            print(f"   ‚úÖ –ü—Ä–æ–µ–∫—Ü–∏—è —É–∂–µ —Å–æ–∑–¥–∞–Ω–∞")
                        elif "index" in query.lower():
                            print(f"   ‚úÖ –ò–Ω–¥–µ–∫—Å —É–∂–µ —Å–æ–∑–¥–∞–Ω")
                        else:
                            print(f"   ‚úÖ –û–±—ä–µ–∫—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
                    else:
                        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –æ—à–∏–±–∫–∞ –≤ –∑–∞–ø—Ä–æ—Å–µ {i}: {e}")
                        print(f"‚ùå –ü—Ä–æ–±–ª–µ–º–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {query}")
                        failed_queries.append((i, query, str(e)))
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü
        target_table_name = config['target_table_name']
        database_name = config['database_name']
        kafka_database = config['kafka_database']
        
        missing_tables = []
        verification_errors = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Kafka-—Ç–∞–±–ª–∏—Ü—ã
        try:
            kafka_tables = client.query(f"SHOW TABLES FROM {kafka_database} LIKE '{target_table_name}_kafka'")
            if kafka_tables.result_rows:
                print(f"‚úÖ Kafka-—Ç–∞–±–ª–∏—Ü–∞ {kafka_database}.{target_table_name}_kafka —Å–æ–∑–¥–∞–Ω–∞")
            else:
                print(f"‚ùå Kafka-—Ç–∞–±–ª–∏—Ü–∞ {kafka_database}.{target_table_name}_kafka –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                missing_tables.append(f"{kafka_database}.{target_table_name}_kafka")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ Kafka-—Ç–∞–±–ª–∏—Ü—ã: {e}")
            verification_errors.append(f"Kafka table check: {e}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
        try:
            local_tables = client.query(f"SHOW TABLES FROM {database_name} LIKE '{target_table_name}_local'")
            if local_tables.result_rows:
                print(f"‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ {database_name}.{target_table_name}_local —Å–æ–∑–¥–∞–Ω–∞")
            else:
                print(f"‚ùå –õ–æ–∫–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ {database_name}.{target_table_name}_local –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                missing_tables.append(f"{database_name}.{target_table_name}_local")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ª–æ–∫–∞–ª—å–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã: {e}")
            verification_errors.append(f"Local table check: {e}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
        try:
            dist_tables = client.query(f"SHOW TABLES FROM {database_name} LIKE '{target_table_name}'")
            if dist_tables.result_rows:
                print(f"‚úÖ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ {database_name}.{target_table_name} —Å–æ–∑–¥–∞–Ω–∞")
            else:
                print(f"‚ùå –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ {database_name}.{target_table_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                missing_tables.append(f"{database_name}.{target_table_name}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã: {e}")
            verification_errors.append(f"Distributed table check: {e}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Materialized View
        try:
            mv_tables = client.query(f"SHOW TABLES FROM {database_name} LIKE '{target_table_name}_mv'")
            if mv_tables.result_rows:
                print(f"‚úÖ Materialized View {database_name}.{target_table_name}_mv —Å–æ–∑–¥–∞–Ω–∞")
            else:
                print(f"‚ùå Materialized View {database_name}.{target_table_name}_mv –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                missing_tables.append(f"{database_name}.{target_table_name}_mv")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ Materialized View: {e}")
            verification_errors.append(f"Materialized View check: {e}")
        
        client.close()
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ–± —É—Å–ø–µ—Ö–µ/–Ω–µ—É–¥–∞—á–µ
        if failed_queries or missing_tables or verification_errors:
            print("\n‚ùå –û–®–ò–ë–ö–ò –û–ë–ù–ê–†–£–ñ–ï–ù–´:")
            if failed_queries:
                print(f"   –ù–µ—É–¥–∞—á–Ω—ã—Ö SQL-–∑–∞–ø—Ä–æ—Å–æ–≤: {len(failed_queries)}")
                for i, query, error in failed_queries:
                    print(f"     - –ó–∞–ø—Ä–æ—Å {i}: {error}")
            if missing_tables:
                print(f"   –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ç–∞–±–ª–∏—Ü: {len(missing_tables)}")
                for table in missing_tables:
                    print(f"     - {table}")
            if verification_errors:
                print(f"   –û—à–∏–±–æ–∫ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏: {len(verification_errors)}")
                for error in verification_errors:
                    print(f"     - {error}")
            
            error_msg = f"SQL –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å —Å –æ—à–∏–±–∫–∞–º–∏: {len(failed_queries)} –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, {len(missing_tables)} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ç–∞–±–ª–∏—Ü, {len(verification_errors)} –æ—à–∏–±–æ–∫ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏"
            raise RuntimeError(error_msg)
        
        print("‚úÖ SQL-—Å–∫—Ä–∏–ø—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ - –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã —Å–æ–∑–¥–∞–Ω—ã")
        return "Success"
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ SQL-—Å–∫—Ä–∏–ø—Ç–∞: {e}")
        raise

def verify_data_flow(**context):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ç–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    try:
        import clickhouse_connect
        import time
        
        config = context['task_instance'].xcom_pull(task_ids='get_table_config', key='table_config')
        
        if not config:
            raise ValueError("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∑–∞–¥–∞—á–∞ get_table_config –≤—ã–ø–æ–ª–Ω–∏–ª–∞—Å—å —É—Å–ø–µ—à–Ω–æ.")
        
        target_table_name = config['target_table_name']
        database_name = config['database_name']
        
        print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ç–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ñ–¥–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        print("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (30 —Å–µ–∫—É–Ω–¥)...")
        time.sleep(30)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ClickHouse
        ch_config = get_clickhouse_config()
        
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse
        client = clickhouse_connect.get_client(**ch_config)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü–µ
        try:
            count_result = client.query(f'SELECT count() FROM {database_name}.{target_table_name}')
            count = count_result.result_rows[0][0]
            print(f"üìä –¢–∞–±–ª–∏—Ü–∞ {database_name}.{target_table_name}: {count} –∑–∞–ø–∏—Å–µ–π")
            
            if count > 0:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
                sample_result = client.query(f'SELECT * FROM {database_name}.{target_table_name} ORDER BY date DESC LIMIT 1')
                if sample_result.result_rows:
                    print(f"üìã –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ {database_name}.{target_table_name}:")
                    print(f"   {sample_result.result_rows[0]}")
            else:
                print(f"‚ö†Ô∏è –í —Ç–∞–±–ª–∏—Ü–µ {database_name}.{target_table_name} –ø–æ–∫–∞ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        
        client.close()
        
        print("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ç–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        return "Success"
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ø–æ—Ç–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise

def health_check(**context):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü"""
    try:
        import clickhouse_connect
        
        config = context['task_instance'].xcom_pull(task_ids='get_table_config', key='table_config')
        
        if not config:
            raise ValueError("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∑–∞–¥–∞—á–∞ get_table_config –≤—ã–ø–æ–ª–Ω–∏–ª–∞—Å—å —É—Å–ø–µ—à–Ω–æ.")
        
        target_table_name = config['target_table_name']
        database_name = config['database_name']
        kafka_database = config['kafka_database']
        
        print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Ç–∞–±–ª–∏—Ü...")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ClickHouse
        ch_config = get_clickhouse_config()
        
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse
        client = clickhouse_connect.get_client(**ch_config)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —Ç–∞–±–ª–∏—Ü
        tables_status = client.query(f'''
            SELECT 
                database,
                table,
                engine,
                total_rows,
                total_bytes
            FROM system.tables 
            WHERE database IN ('{database_name}', '{kafka_database}')
            AND table LIKE '{target_table_name}%'
            ORDER BY database, table
        ''')
        
        print("üìã –°—Ç–∞—Ç—É—Å —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü:")
        for row in tables_status.result_rows:
            print(f"   {row[0]}.{row[1]} ({row[2]}): {row[3]} —Å—Ç—Ä–æ–∫, {row[4]} –±–∞–π—Ç")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Materialized Views
        mv_status = client.query(f'''
            SELECT 
                database,
                table,
                engine,
                engine_full
            FROM system.tables 
            WHERE engine = 'MaterializedView' 
            AND database IN ('{database_name}', '{kafka_database}')
            AND table LIKE '{target_table_name}%'
        ''')
        
        print("üìã Materialized Views:")
        for row in mv_status.result_rows:
            print(f"   {row[0]}.{row[1]} ({row[2]})")
        
        client.close()
        
        print("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Ç–∞–±–ª–∏—Ü –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        return "Healthy"
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∑–¥–æ—Ä–æ–≤—å—è —Ç–∞–±–ª–∏—Ü: {e}")
        raise

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á
get_config_task = PythonOperator(
    task_id='get_table_config',
    python_callable=get_table_config,
    dag=kafka_to_ch_dag,
)

check_connections_task = PythonOperator(
    task_id='check_connections',
    python_callable=check_connections,
    dag=kafka_to_ch_dag,
)

generate_sql_task = PythonOperator(
    task_id='generate_sql_script',
    python_callable=generate_sql_script,
    dag=kafka_to_ch_dag,
)

execute_sql_task = PythonOperator(
    task_id='execute_sql_script',
    python_callable=execute_sql_script,
    dag=kafka_to_ch_dag,
)

verify_flow_task = PythonOperator(
    task_id='verify_data_flow',
    python_callable=verify_data_flow,
    dag=kafka_to_ch_dag,
)

health_check_task = PythonOperator(
    task_id='health_check',
    python_callable=health_check,
    dag=kafka_to_ch_dag,
)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
get_config_task >> check_connections_task >> generate_sql_task >> execute_sql_task >> verify_flow_task >> health_check_task
