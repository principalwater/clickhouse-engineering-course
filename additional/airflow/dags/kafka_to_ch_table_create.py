from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
import json
import os
import sys
from dotenv import load_dotenv

# Load environment variables
dotenv_path = os.path.join(os.path.dirname(__file__), '../../infra/env/clickhouse.env')
load_dotenv(dotenv_path=dotenv_path)

# ĞŸĞ¾Ğ»Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¸Ğ· Kafka Ğ² ClickHouse
# Ğ­Ñ‚Ğ¸ Ğ¿Ğ¾Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ¸Ğ· String Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ‚Ğ¸Ğ¿Ñ‹
# Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¿Ğ°Ñ€Ñ‚Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸ ClickHouse
DATE_FIELDS = ['date', 'event_date', 'created_date', 'updated_date']  # String -> Date
DATETIME_FIELDS = ['timestamp', 'created_at', 'updated_at', 'event_time']  # String -> DateTime

def fix_projection_order_by(order_by_clause):
    """
    Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ ORDER BY Ğ´Ğ»Ñ ClickHouse projection.
    Ğ’ projection ClickHouse Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ²Ğ½Ñ‹Ğµ ASC/DESC Ğ² Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸ÑĞµ.
    Ğ”Ğ»Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¹ Ñ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ.
    """
    if not order_by_clause:
        return order_by_clause
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞµÑÑ‚ÑŒ Ğ»Ğ¸ DESC Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ
    if 'DESC' in order_by_clause.upper():
        # Ğ”Ğ»Ñ projection Ñ DESC Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ±ĞµĞ· Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹
        # Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ»Ñ Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼
        fields = [field.strip() for field in order_by_clause.split(',')]
        clean_fields = []
        
        for field in fields:
            # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ASC/DESC Ğ¸Ğ· Ğ¿Ğ¾Ğ»Ñ, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ¼Ñ
            clean_field = field.replace(' DESC', '').replace(' ASC', '').strip()
            clean_fields.append(clean_field)
        
        result = ', '.join(clean_fields)
        if result != order_by_clause:
            print(f"ğŸ”§ Ğ£Ğ¿Ñ€Ğ¾Ñ‰ĞµĞ½ ORDER BY Ğ´Ğ»Ñ projection (ÑƒĞ±Ñ€Ğ°Ğ½Ñ‹ ASC/DESC): '{order_by_clause}' â†’ '{result}'")
        return result
    
    return order_by_clause

def check_existing_tables(client, target_table_name, database_name, kafka_database):
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†"""
    existing_tables = []
    
    # Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸
    tables_to_check = [
        (kafka_database, f"{target_table_name}_kafka"),
        (database_name, f"{target_table_name}_local"),
        (database_name, target_table_name),
        (database_name, f"{target_table_name}_mv")
    ]
    
    for db, table in tables_to_check:
        try:
            result = client.query(f"EXISTS {db}.{table}")
            if result.result_rows and result.result_rows[0][0] == 1:
                existing_tables.append(f"{db}.{table}")
        except Exception:
            # Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° Ğ½Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°
            pass
    
    return existing_tables

def get_clickhouse_config():
    """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ClickHouse Ğ¸Ğ· Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ"""
    host = os.getenv("CLICKHOUSE_HOST")
    port = os.getenv("CLICKHOUSE_PORT")
    username = os.getenv("CH_USER")
    password = os.getenv("CH_PASSWORD")
    
    missing_vars = []
    if not host:
        missing_vars.append("CLICKHOUSE_HOST")
    if not port:
        missing_vars.append("CLICKHOUSE_PORT")
    if not username:
        missing_vars.append("CH_USER")
    if not password:
        missing_vars.append("CH_PASSWORD")
    
    if missing_vars:
        raise ValueError(f"ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ClickHouse: {', '.join(missing_vars)}. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ñ„Ğ°Ğ¹Ğ» infra/env/clickhouse.env")
    
    return {
        'host': host,
        'port': int(port),
        'username': username,
        'password': password,
        'secure': False
    }

# ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² DAG
default_args = {
    'owner': 'energy-hub',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ DAG Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¸Ğ· Kafka Ğ² ClickHouse
kafka_to_ch_dag = DAG(
    'kafka_to_ch_table_create',
    default_args=default_args,
    description='Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† ClickHouse Ğ¸Ğ· Kafka Ñ‚Ğ¾Ğ¿Ğ¸ĞºĞ¾Ğ² (Kafka â†’ Materialized View â†’ Distributed)',
    schedule=None,  # Ğ ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº
    catchup=False,
    tags=['clickhouse', 'kafka', 'tables', 'kafka-to-clickhouse'],
)

def get_table_config(**context):
    """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ¸Ğ· Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² DAG"""
    try:
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° DAG
        dag_run = context['dag_run']
        conf = dag_run.conf if dag_run else {}
        
        # ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ…ĞµĞ¼Ñ‹ Kafka â†’ Materialized View â†’ Distributed Ğ² DWH Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ
        default_config = {
            'kafka_topic': 'covid_new_cases_1min',
            'target_table_name': 'covid_new_cases',
            'dwh_layer': 'raw',  # raw, ods, dds, cdm
            'kafka_database': 'otus_kafka',  # Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Kafka Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†
            'sort_key': 'date, location_key',
            'partition_key': 'toYYYYMM(date)',
            'shard_key': 'xxHash64(location_key)',
            'cluster_name': 'dwh_test',
            'kafka_broker': 'kafka:9092',
            # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
            'create_projection': True,  # ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
            'projection_order_by': None,  # ĞµÑĞ»Ğ¸ None, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ sort_key
            'create_indexes': True,  # ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑÑ‹
            'index_fields': [],  # Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ¾Ğ², Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑÑ‚Ğ¾
            'table_settings': {},  # Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹
            'skip_alter_on_error': True,  # Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ ALTER Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…
            'recreate_tables': False,  # Ğ¿ĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ (ÑƒĞ´Ğ°Ğ»ÑÑ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ)
            'schema': {
                'date': 'String',
                'location_key': 'String',
                'new_confirmed': 'Int32',
                'new_deceased': 'Int32',
                'new_recovered': 'Int32',
                'new_tested': 'Int32'
            }
        }
        
        # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸
        config = {**default_config, **conf}
        
        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»Ğ¾Ñ DWH
        dwh_layer = config['dwh_layer']
        if dwh_layer == 'raw':
            database_name = 'raw'
        elif dwh_layer == 'ods':
            database_name = 'ods'
        elif dwh_layer == 'dds':
            database_name = 'dds'
        elif dwh_layer == 'cdm':
            database_name = 'cdm'
        else:
            database_name = 'raw'  # Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ
        
        config['database_name'] = database_name
        
        print(f"ğŸ“‹ ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ¿Ğ¾ ÑÑ…ĞµĞ¼Ğµ Kafka â†’ Materialized View â†’ Distributed:")
        print(f"   Kafka Ñ‚Ğ¾Ğ¿Ğ¸Ğº: {config['kafka_topic']}")
        print(f"   Ğ¦ĞµĞ»ĞµĞ²Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°: {config['target_table_name']}")
        print(f"   Ğ¡Ğ»Ğ¾Ğ¹ DWH: {config['dwh_layer']}")
        print(f"   Ğ‘Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DWH: {config['database_name']}")
        print(f"   Ğ‘Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Kafka: {config['kafka_database']}")
        print(f"   ĞšĞ»ÑÑ‡ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸: {config['sort_key']}")
        print(f"   ĞšĞ»ÑÑ‡ Ğ¿Ğ°Ñ€Ñ‚Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: {config['partition_key']}")
        print(f"   ĞšĞ»ÑÑ‡ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: {config['shard_key']}")
        print(f"   ĞšĞ»Ğ°ÑÑ‚ĞµÑ€: {config['cluster_name']}")
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ² XCom Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…
        context['task_instance'].xcom_push(key='table_config', value=config)
        
        return config
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸: {e}")
        raise

def check_connections(**context):
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹ Ğº ClickHouse Ğ¸ Kafka"""
    try:
        import clickhouse_connect
        from kafka import KafkaConsumer
        
        config = context['task_instance'].xcom_pull(task_ids='get_table_config', key='table_config')
        
        if not config:
            raise ValueError("ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° get_table_config Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ»Ğ°ÑÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾.")
        
        print("ğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹...")
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ClickHouse
        ch_config = get_clickhouse_config()
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ClickHouse Ñ‡ĞµÑ€ĞµĞ· HTTP Ğ¿Ğ¾Ñ€Ñ‚
        client = clickhouse_connect.get_client(**ch_config)
        
        result = client.query('SELECT version()')
        version = result.result_rows[0][0]
        print(f"âœ… ClickHouse: Ğ²ĞµÑ€ÑĞ¸Ñ {version}")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
        cluster_result = client.query(f"SELECT name, host_name, port FROM system.clusters WHERE name = '{config['cluster_name']}'")
        if cluster_result.result_rows:
            print(f"âœ… ĞšĞ»Ğ°ÑÑ‚ĞµÑ€ {config['cluster_name']}: {len(cluster_result.result_rows)} ÑƒĞ·Ğ»Ğ¾Ğ²")
        else:
            print(f"âš ï¸ ĞšĞ»Ğ°ÑÑ‚ĞµÑ€ {config['cluster_name']} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
        
        client.close()
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Kafka
        consumer = KafkaConsumer(
            bootstrap_servers=[config['kafka_broker']],
            consumer_timeout_ms=5000
        )
        
        topics = consumer.topics()
        if config['kafka_topic'] in topics:
            print(f"âœ… Kafka Ñ‚Ğ¾Ğ¿Ğ¸Ğº {config['kafka_topic']} Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
        else:
            print(f"âš ï¸ Kafka Ñ‚Ğ¾Ğ¿Ğ¸Ğº {config['kafka_topic']} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
        
        consumer.close()
        
        return "Success"
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹: {e}")
        raise

def generate_sql_script(**context):
    """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸"""
    try:
        config = context['task_instance'].xcom_pull(task_ids='get_table_config', key='table_config')
        
        if not config:
            raise ValueError("ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° get_table_config Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ»Ğ°ÑÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾.")
        
        print("ğŸ”„ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°...")
        
        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ ÑÑ…ĞµĞ¼Ñ‹ Kafka â†’ Materialized View â†’ Distributed
        kafka_topic = config['kafka_topic']
        target_table_name = config['target_table_name']
        database_name = config['database_name']
        kafka_database = config['kafka_database']
        dwh_layer = config['dwh_layer']
        sort_key = config['sort_key']
        partition_key = config['partition_key']
        shard_key = config['shard_key']
        cluster_name = config['cluster_name']
        kafka_broker = config['kafka_broker']
        schema = config['schema']
        
        # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ¿Ğ¾ ÑÑ…ĞµĞ¼Ğµ Kafka â†’ Materialized View â†’ Distributed Ğ² DWH Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ
        sql_script = f"""
-- =====================================================
-- ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´Ğ»Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ {target_table_name}
-- Ğ¡Ñ…ĞµĞ¼Ğ°: Kafka Topic â†’ Kafka Table Engine â†’ Materialized View â†’ ReplicatedMergeTree/Distributed
-- Ğ¡Ğ»Ğ¾Ğ¹ DWH: {dwh_layer}
-- Ğ¢Ğ¾Ğ¿Ğ¸Ğº: {kafka_topic}
-- Ğ‘Ğ°Ğ·Ğ° Kafka: {kafka_database}
-- ĞŸĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†: {config.get('recreate_tables', False)}
-- =====================================================
"""
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹
        if config.get('recreate_tables', False):
            sql_script += f"""

-- Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ
DROP TABLE IF EXISTS {database_name}.{target_table_name}_mv ON CLUSTER {cluster_name};
DROP TABLE IF EXISTS {database_name}.{target_table_name} ON CLUSTER {cluster_name};
DROP TABLE IF EXISTS {database_name}.{target_table_name}_local ON CLUSTER {cluster_name};
DROP TABLE IF EXISTS {kafka_database}.{target_table_name}_kafka ON CLUSTER {cluster_name};

"""
        
        sql_script += f"""
-- Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ DWH Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹
CREATE DATABASE IF NOT EXISTS {kafka_database} ON CLUSTER {cluster_name};
CREATE DATABASE IF NOT EXISTS raw ON CLUSTER {cluster_name};
CREATE DATABASE IF NOT EXISTS ods ON CLUSTER {cluster_name};
CREATE DATABASE IF NOT EXISTS dds ON CLUSTER {cluster_name};
CREATE DATABASE IF NOT EXISTS cdm ON CLUSTER {cluster_name};

-- 1. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ñ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ¼ Kafka Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ‚Ğ¾Ğ¿Ğ¸ĞºĞ°
CREATE TABLE IF NOT EXISTS {kafka_database}.{target_table_name}_kafka ON CLUSTER {cluster_name} (
"""
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ğ¾Ğ»Ñ ÑÑ…ĞµĞ¼Ñ‹
        for field_name, field_type in schema.items():
            sql_script += f"    {field_name} {field_type},\n"
        
        sql_script = sql_script.rstrip(',\n') + f"""
) ENGINE = Kafka
SETTINGS
    kafka_broker_list = '{kafka_broker}',
    kafka_topic_list = '{kafka_topic}',
    kafka_group_name = 'clickhouse-{target_table_name}-consumer',
    kafka_format = 'JSONEachRow',
    kafka_num_consumers = 1,
    kafka_skip_broken_messages = 1000,
    kafka_row_delimiter = '\\n';

-- 2. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ñ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ¼ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° MergeTree (ReplicatedMergeTree)
CREATE TABLE IF NOT EXISTS {database_name}.{target_table_name}_local ON CLUSTER {cluster_name} (
"""
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ğ¾Ğ»Ñ ÑÑ…ĞµĞ¼Ñ‹ Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¸Ğ¿Ğ¾Ğ²
        for field_name, field_type in schema.items():
            if field_name in DATETIME_FIELDS:
                sql_script += f"    {field_name} DateTime,\n"
            elif field_name in DATE_FIELDS and field_type == 'String':
                sql_script += f"    {field_name} Date,\n"
            else:
                sql_script += f"    {field_name} {field_type},\n"
        
        sql_script = sql_script.rstrip(',\n') + f"""
) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{{shard}}/{database_name}/{target_table_name}_local/{{uuid}}/', '{{replica}}')
PARTITION BY {partition_key}
ORDER BY ({sort_key})
PRIMARY KEY ({sort_key});

-- Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°
CREATE TABLE IF NOT EXISTS {database_name}.{target_table_name} ON CLUSTER {cluster_name} (
"""
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ğ¾Ğ»Ñ ÑÑ…ĞµĞ¼Ñ‹ Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¸Ğ¿Ğ¾Ğ²
        for field_name, field_type in schema.items():
            if field_name in DATETIME_FIELDS:
                sql_script += f"    {field_name} DateTime,\n"
            elif field_name in DATE_FIELDS and field_type == 'String':
                sql_script += f"    {field_name} Date,\n"
            else:
                sql_script += f"    {field_name} {field_type},\n"
        
        sql_script = sql_script.rstrip(',\n') + f"""
) ENGINE = Distributed('{cluster_name}', '{database_name}', '{target_table_name}_local', {shard_key});

-- Materialized View
CREATE MATERIALIZED VIEW IF NOT EXISTS {database_name}.{target_table_name}_mv ON CLUSTER {cluster_name} 
TO {database_name}.{target_table_name} AS
SELECT
"""
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ğ¾Ğ»Ñ Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¸Ğ¿Ğ¾Ğ²
        for field_name, field_type in schema.items():
            if field_name in DATETIME_FIELDS:
                sql_script += f"    parseDateTimeBestEffort({field_name}) AS {field_name},\n"
            elif field_name in DATE_FIELDS and field_type == 'String':
                sql_script += f"    parseDateTimeBestEffort({field_name}) AS {field_name},\n"
            else:
                sql_script += f"    {field_name},\n"
        
        sql_script = sql_script.rstrip(',\n') + f"""
FROM {kafka_database}.{target_table_name}_kafka;
"""
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¾)
        if config.get('create_indexes', True) and config.get('index_fields'):
            for field in config['index_fields']:
                if field in schema:
                    sql_script += f"""
-- Ğ˜Ğ½Ğ´ĞµĞºÑ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ»Ñ {field}
ALTER TABLE {database_name}.{target_table_name}_local ON CLUSTER {cluster_name} 
ADD INDEX IF NOT EXISTS idx_{field} {field} TYPE minmax GRANULARITY 4;

"""
        
        if config.get('create_projection', True):
            projection_order = config.get('projection_order_by') or sort_key
            
            # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ORDER BY Ğ´Ğ»Ñ projection
            # ClickHouse Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ
            projection_order = fix_projection_order_by(projection_order)
            sql_script += f"""
-- ĞŸÑ€Ğ¾ĞµĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
ALTER TABLE {database_name}.{target_table_name}_local ON CLUSTER {cluster_name} 
ADD PROJECTION IF NOT EXISTS {target_table_name}_projection (
    SELECT * ORDER BY {projection_order}
);

-- ĞœĞ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸
ALTER TABLE {database_name}.{target_table_name}_local ON CLUSTER {cluster_name} 
MATERIALIZE PROJECTION {target_table_name}_projection;

"""
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ² XCom
        context['task_instance'].xcom_push(key='sql_script', value=sql_script)
        
        print(f"âœ… SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ»Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ {target_table_name}")
        print(f"ğŸ“‹ Ğ Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°: {len(sql_script)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²")
        print(f"ğŸ“‹ ĞŸĞµÑ€Ğ²Ñ‹Ğµ 500 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²: {sql_script[:500]}")
        
        return "Success"
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°: {e}")
        raise

def execute_sql_script(**context):
    """Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°"""
    try:
        import clickhouse_connect
        
        config = context['task_instance'].xcom_pull(task_ids='get_table_config', key='table_config')
        sql_script = context['task_instance'].xcom_pull(task_ids='generate_sql_script', key='sql_script')
        
        if not config:
            raise ValueError("ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° get_table_config Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ»Ğ°ÑÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾.")
        
        if not sql_script:
            raise ValueError("SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° generate_sql_script Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ»Ğ°ÑÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾.")
        
        print("ğŸ”„ Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°...")
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ClickHouse
        ch_config = get_clickhouse_config()
        
        # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº ClickHouse
        client = clickhouse_connect.get_client(**ch_config)
        
        # Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ° Ğ½Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ (Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ)
        # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¸ Ğ¸ Ğ¿ÑƒÑÑ‚Ñ‹Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸
        lines = []
        for line in sql_script.split('\n'):
            line = line.strip()
            if line and not line.startswith('--'):
                lines.append(line)
        
        # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹
        queries = []
        current_query = []
        in_multiline = False
        
        for line in lines:
            if line.startswith('CREATE') or line.startswith('ALTER') or line.startswith('DROP'):
                # ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
                if current_query:
                    queries.append(' '.join(current_query))
                current_query = [line]
                in_multiline = True
            elif in_multiline:
                current_query.append(line)
                if line.endswith(';'):
                    # Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ÑÑ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
                    queries.append(' '.join(current_query))
                    current_query = []
                    in_multiline = False
            else:
                # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
                if line.endswith(';'):
                    queries.append(line)
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ, ĞµÑĞ»Ğ¸ Ğ¾Ğ½ ĞµÑÑ‚ÑŒ
        if current_query:
            queries.append(' '.join(current_query))
        
        print(f"ğŸ“‹ Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ {len(queries)} SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²...")
        print(f"ğŸ“‹ Ğ Ğ°Ğ·Ğ¼ĞµÑ€ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°: {len(sql_script)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²")
        print(f"ğŸ“‹ ĞŸĞµÑ€Ğ²Ñ‹Ğµ 500 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² SQL: {sql_script[:500]}")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¿ĞµÑ€ĞµĞ´ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼
        target_table_name = config['target_table_name']
        database_name = config['database_name'] 
        kafka_database = config['kafka_database']
        recreate_tables = config.get('recreate_tables', False)
        
        existing_tables = check_existing_tables(client, target_table_name, database_name, kafka_database)
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ğ²ÑĞµ Ğ»Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‚
        expected_tables = [
            f"{kafka_database}.{target_table_name}_kafka",
            f"{database_name}.{target_table_name}_local", 
            f"{database_name}.{target_table_name}",
            f"{database_name}.{target_table_name}_mv"
        ]
        
        all_tables_exist = all(table in existing_tables for table in expected_tables)
        
        if not recreate_tables and existing_tables:
            print("â„¹ï¸  Ğ˜ĞĞ¤ĞĞ ĞœĞĞ¦Ğ˜Ğ¯ Ğ Ğ¡Ğ£Ğ©Ğ•Ğ¡Ğ¢Ğ’Ğ£Ğ®Ğ©Ğ˜Ğ¥ Ğ¢ĞĞ‘Ğ›Ğ˜Ğ¦ĞĞ¥:")
            for table_info in existing_tables:
                print(f"   âœ… {table_info} ÑƒĞ¶Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚")
            
            if all_tables_exist:
                print("ğŸ¯ Ğ’ÑĞµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ ÑƒĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²")
                print("âœ… SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾: Ğ²ÑĞµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ ÑƒĞ¶Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‚")
                
                # Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµĞ¼ Ñ ÑƒÑĞ¿ĞµÑ…Ğ¾Ğ¼
                client.close()
                return {
                    'status': 'success_skip',
                    'message': 'Ğ’ÑĞµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ ÑƒĞ¶Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‚',
                    'existing_tables': existing_tables,
                    'skipped_queries': len(queries)
                }
        
        # ĞÑ‚Ğ»Ğ°Ğ´Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ñ…
        for i, query in enumerate(queries, 1):
            print(f"ğŸ“‹ Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ {i}: {query[:100]}...")
        
        failed_queries = []
        for i, query in enumerate(queries, 1):
            if query.strip():
                try:
                    print(f"ğŸ”„ Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ {i}/{len(queries)}: {query[:100]}...")
                    client.command(query)
                    print(f"âœ… Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ {i} Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½")
                except Exception as e:
                    error_str = str(e)
                    
                    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ»Ğ¸ ÑÑ‚Ğ¾ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¾Ğ¹ Ğ¾ ÑƒĞ¶Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ…
                    is_already_exists = (
                        "already exists" in error_str.lower() or 
                        "ILLEGAL_COLUMN" in error_str or  # Ğ¸Ğ½Ğ´ĞµĞºÑ ÑƒĞ¶Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚
                        "ILLEGAL_PROJECTION" in error_str or  # Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ ÑƒĞ¶Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚
                        "583" in error_str  # ĞºĞ¾Ğ´ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ´Ğ»Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸
                    )
                    
                    if is_already_exists and not recreate_tables:
                        # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ğº ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹ Ğ´Ğ»Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²
                        print(f"â„¹ï¸  Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ {i}: Ğ¾Ğ±ÑŠĞµĞºÑ‚ ÑƒĞ¶Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼")
                        if "projection" in query.lower():
                            print(f"   âœ… ĞŸÑ€Ğ¾ĞµĞºÑ†Ğ¸Ñ ÑƒĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ°")
                        elif "index" in query.lower():
                            print(f"   âœ… Ğ˜Ğ½Ğ´ĞµĞºÑ ÑƒĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½")
                        else:
                            print(f"   âœ… ĞĞ±ÑŠĞµĞºÑ‚ ÑƒĞ¶Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚")
                    else:
                        print(f"âŒ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ {i}: {e}")
                        print(f"âŒ ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ: {query}")
                        failed_queries.append((i, query, str(e)))
                    # ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†
        target_table_name = config['target_table_name']
        database_name = config['database_name']
        kafka_database = config['kafka_database']
        
        missing_tables = []
        verification_errors = []
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Kafka-Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹
        try:
            kafka_tables = client.query(f"SHOW TABLES FROM {kafka_database} LIKE '{target_table_name}_kafka'")
            if kafka_tables.result_rows:
                print(f"âœ… Kafka-Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° {kafka_database}.{target_table_name}_kafka ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ°")
            else:
                print(f"âŒ Kafka-Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° {kafka_database}.{target_table_name}_kafka Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
                missing_tables.append(f"{kafka_database}.{target_table_name}_kafka")
        except Exception as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Kafka-Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹: {e}")
            verification_errors.append(f"Kafka table check: {e}")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹
        try:
            local_tables = client.query(f"SHOW TABLES FROM {database_name} LIKE '{target_table_name}_local'")
            if local_tables.result_rows:
                print(f"âœ… Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° {database_name}.{target_table_name}_local ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ°")
            else:
                print(f"âŒ Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° {database_name}.{target_table_name}_local Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
                missing_tables.append(f"{database_name}.{target_table_name}_local")
        except Exception as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹: {e}")
            verification_errors.append(f"Local table check: {e}")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹
        try:
            dist_tables = client.query(f"SHOW TABLES FROM {database_name} LIKE '{target_table_name}'")
            if dist_tables.result_rows:
                print(f"âœ… Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° {database_name}.{target_table_name} ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ°")
            else:
                print(f"âŒ Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° {database_name}.{target_table_name} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
                missing_tables.append(f"{database_name}.{target_table_name}")
        except Exception as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹: {e}")
            verification_errors.append(f"Distributed table check: {e}")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Materialized View
        try:
            mv_tables = client.query(f"SHOW TABLES FROM {database_name} LIKE '{target_table_name}_mv'")
            if mv_tables.result_rows:
                print(f"âœ… Materialized View {database_name}.{target_table_name}_mv ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ°")
            else:
                print(f"âŒ Materialized View {database_name}.{target_table_name}_mv Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
                missing_tables.append(f"{database_name}.{target_table_name}_mv")
        except Exception as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Materialized View: {e}")
            verification_errors.append(f"Materialized View check: {e}")
        
        client.close()
        
        # ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ± ÑƒÑĞ¿ĞµÑ…Ğµ/Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğµ
        if failed_queries or missing_tables or verification_errors:
            print("\nâŒ ĞĞ¨Ğ˜Ğ‘ĞšĞ˜ ĞĞ‘ĞĞĞ Ğ£Ğ–Ğ•ĞĞ«:")
            if failed_queries:
                print(f"   ĞĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²: {len(failed_queries)}")
                for i, query, error in failed_queries:
                    print(f"     - Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ {i}: {error}")
            if missing_tables:
                print(f"   ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†: {len(missing_tables)}")
                for table in missing_tables:
                    print(f"     - {table}")
            if verification_errors:
                print(f"   ĞÑˆĞ¸Ğ±Ğ¾Ğº Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸: {len(verification_errors)}")
                for error in verification_errors:
                    print(f"     - {error}")
            
            error_msg = f"SQL Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ğ»Ğ¾ÑÑŒ Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸: {len(failed_queries)} Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², {len(missing_tables)} Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†, {len(verification_errors)} Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸"
            raise RuntimeError(error_msg)
        
        print("âœ… SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ - Ğ²ÑĞµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹")
        return "Success"
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°: {e}")
        raise

def verify_data_flow(**context):
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
    try:
        import clickhouse_connect
        import time
        
        config = context['task_instance'].xcom_pull(task_ids='get_table_config', key='table_config')
        
        if not config:
            raise ValueError("ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° get_table_config Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ»Ğ°ÑÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾.")
        
        target_table_name = config['target_table_name']
        database_name = config['database_name']
        
        print("ğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…...")
        
        # Ğ–Ğ´ĞµĞ¼ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        print("â³ ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (30 ÑĞµĞºÑƒĞ½Ğ´)...")
        time.sleep(30)
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ClickHouse
        ch_config = get_clickhouse_config()
        
        # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº ClickHouse
        client = clickhouse_connect.get_client(**ch_config)
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ
        try:
            count_result = client.query(f'SELECT count() FROM {database_name}.{target_table_name}')
            count = count_result.result_rows[0][0]
            print(f"ğŸ“Š Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° {database_name}.{target_table_name}: {count} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹")
            
            if count > 0:
                # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
                sample_result = client.query(f'SELECT * FROM {database_name}.{target_table_name} ORDER BY date DESC LIMIT 1')
                if sample_result.result_rows:
                    print(f"ğŸ“‹ ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· {database_name}.{target_table_name}:")
                    print(f"   {sample_result.result_rows[0]}")
            else:
                print(f"âš ï¸ Ğ’ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ {database_name}.{target_table_name} Ğ¿Ğ¾ĞºĞ° Ğ½ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
                
        except Exception as e:
            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {e}")
        
        client.close()
        
        print("âœ… ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°")
        return "Success"
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {e}")
        raise

def health_check(**context):
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†"""
    try:
        import clickhouse_connect
        
        config = context['task_instance'].xcom_pull(task_ids='get_table_config', key='table_config')
        
        if not config:
            raise ValueError("ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° get_table_config Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ»Ğ°ÑÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾.")
        
        target_table_name = config['target_table_name']
        database_name = config['database_name']
        kafka_database = config['kafka_database']
        
        print("ğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†...")
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ClickHouse
        ch_config = get_clickhouse_config()
        
        # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº ClickHouse
        client = clickhouse_connect.get_client(**ch_config)
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑ‚Ğ°Ñ‚ÑƒÑĞ° Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†
        tables_status = client.query(f'''
            SELECT 
                database,
                table,
                engine,
                total_rows,
                total_bytes
            FROM system.tables 
            WHERE database IN ('{database_name}', '{kafka_database}')
            AND table LIKE '{target_table_name}%'
            ORDER BY database, table
        ''')
        
        print("ğŸ“‹ Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†:")
        for row in tables_status.result_rows:
            print(f"   {row[0]}.{row[1]} ({row[2]}): {row[3]} ÑÑ‚Ñ€Ğ¾Ğº, {row[4]} Ğ±Ğ°Ğ¹Ñ‚")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Materialized Views
        mv_status = client.query(f'''
            SELECT 
                database,
                table,
                engine,
                engine_full
            FROM system.tables 
            WHERE engine = 'MaterializedView' 
            AND database IN ('{database_name}', '{kafka_database}')
            AND table LIKE '{target_table_name}%'
        ''')
        
        print("ğŸ“‹ Materialized Views:")
        for row in mv_status.result_rows:
            print(f"   {row[0]}.{row[1]} ({row[2]})")
        
        client.close()
        
        print("âœ… ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°")
        return "Healthy"
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†: {e}")
        raise

# ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡
get_config_task = PythonOperator(
    task_id='get_table_config',
    python_callable=get_table_config,
    dag=kafka_to_ch_dag,
)

check_connections_task = PythonOperator(
    task_id='check_connections',
    python_callable=check_connections,
    dag=kafka_to_ch_dag,
)

generate_sql_task = PythonOperator(
    task_id='generate_sql_script',
    python_callable=generate_sql_script,
    dag=kafka_to_ch_dag,
)

execute_sql_task = PythonOperator(
    task_id='execute_sql_script',
    python_callable=execute_sql_script,
    dag=kafka_to_ch_dag,
)

verify_flow_task = PythonOperator(
    task_id='verify_data_flow',
    python_callable=verify_data_flow,
    dag=kafka_to_ch_dag,
)

health_check_task = PythonOperator(
    task_id='health_check',
    python_callable=health_check,
    dag=kafka_to_ch_dag,
)

# ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹
get_config_task >> check_connections_task >> generate_sql_task >> execute_sql_task >> verify_flow_task >> health_check_task
