from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
import json
import os
import logging
from dotenv import load_dotenv

# Load environment variables
dotenv_path = os.path.join(os.path.dirname(__file__), '../../infra/env/clickhouse.env')
load_dotenv(dotenv_path=dotenv_path)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è DAG
default_args = {
    'owner': 'hw17-manual-consumer',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
    'max_active_runs': 1,  # –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –∑–∞–ø—É—Å–∫ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
}

def setup_manual_consumer_table(**context):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ Kafka Engine"""
    try:
        import clickhouse_connect
        
        print("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ClickHouse –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
        ch_config = {
            'host': os.getenv('CLICKHOUSE_HOST', 'localhost'),
            'port': int(os.getenv('CLICKHOUSE_PORT', 8123)),
            'username': os.getenv('CH_USER'),
            'password': os.getenv('CH_PASSWORD'),
            'secure': False
        }
        
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse
        client = clickhouse_connect.get_client(**ch_config)
        
        # SQL –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã –±–µ–∑ Kafka Engine
        create_sql = """
        CREATE TABLE IF NOT EXISTS raw.covid_manual_consumption ON CLUSTER dwh_test (
            -- –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è –¥–∞–Ω–Ω—ã—Ö
            date Date,
            location_key LowCardinality(String),
            new_confirmed Int32,
            new_deceased Int32,
            new_recovered Int32,
            new_tested Int32,
            
            -- –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä—É—á–Ω–æ–≥–æ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è
            batch_id String,
            kafka_partition Int32,
            kafka_offset Int64,
            processing_time DateTime DEFAULT now(),
            consumer_group_id String
        ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/raw/covid_manual_consumption/{uuid}', '{replica}')
        PARTITION BY toYYYYMM(date)
        ORDER BY (date, location_key)
        SETTINGS index_granularity = 8192;
        """
        
        client.command(create_sql)
        print("‚úÖ –¢–∞–±–ª–∏—Ü–∞ raw.covid_manual_consumption —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        
        # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã
        distributed_sql = """
        CREATE TABLE IF NOT EXISTS raw.covid_manual_consumption_dist ON CLUSTER dwh_test AS raw.covid_manual_consumption
        ENGINE = Distributed('dwh_test', 'raw', 'covid_manual_consumption', xxHash64(location_key));
        """
        
        try:
            client.command(distributed_sql)
            print("‚úÖ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ covid_manual_consumption_dist —Å–æ–∑–¥–∞–Ω–∞")
        except Exception as e:
            print(f"‚ö†Ô∏è –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ —Å–æ–∑–¥–∞–Ω–∞ (–≤–æ–∑–º–æ–∂–Ω–æ, –∫–ª–∞—Å—Ç–µ—Ä –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω): {e}")
        
        client.close()
        
        return {
            'table_created': True,
            'target_table': 'raw.covid_manual_consumption',
            'distributed_table': 'raw.covid_manual_consumption_dist'
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–∞–±–ª–∏—Ü—ã: {e}")
        raise

def consume_kafka_messages_batch(**context):
    """
    –ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –±–∞—Ç—á–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ Kafka –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Kafka Engine
    
    –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–º—É Kafka Engine ClickHouse:
    - –ü—Ä—è–º–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Kafka —á–µ—Ä–µ–∑ kafka-python
    - –†—É—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å offset'–æ–≤  
    - –ë–∞—Ç—á–µ–≤–∞—è –≤—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ ClickHouse
    - –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    try:
        from kafka import KafkaConsumer
        import clickhouse_connect
        
        processing_time = 0
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        batch_size = context.get('params', {}).get('batch_size', 50)
        timeout_seconds = context.get('params', {}).get('timeout_seconds', 60)
        topic = context.get('params', {}).get('topic', 'covid_new_cases_1min')
        
        print(f"üöÄ –ó–∞–ø—É—Å–∫ —Ä—É—á–Ω–æ–≥–æ –∫–æ–Ω—Å—å—é–º–µ—Ä–∞ Kafka")
        print(f"   Topic: {topic}")
        print(f"   Batch size: {batch_size}")
        print(f"   Timeout: {timeout_seconds}s")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Kafka
        kafka_broker = os.getenv('KAFKA_BROKER', 'kafka:9092')
        
        # –°–æ–∑–¥–∞–µ–º Kafka –∫–æ–Ω—Å—å—é–º–µ—Ä
        consumer = KafkaConsumer(
            topic,
            bootstrap_servers=[kafka_broker],
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            group_id=f'manual-clickhouse-consumer-{topic}',
            auto_offset_reset='latest',
            consumer_timeout_ms=timeout_seconds * 1000,
            enable_auto_commit=True
        )
        
        print(f"‚úÖ Kafka –∫–æ–Ω—Å—å—é–º–µ—Ä –ø–æ–¥–∫–ª—é—á–µ–Ω –∫ {kafka_broker}")
        
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse
        ch_config = {
            'host': os.getenv('CLICKHOUSE_HOST', 'localhost'),
            'port': int(os.getenv('CLICKHOUSE_PORT', 8123)),
            'username': os.getenv('CH_USER'),
            'password': os.getenv('CH_PASSWORD'),
            'secure': False
        }
        
        ch_client = clickhouse_connect.get_client(**ch_config)
        print("‚úÖ ClickHouse –∫–ª–∏–µ–Ω—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω")
        
        # –°–±–æ—Ä –±–∞—Ç—á–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
        batch = []
        batch_id = datetime.now().strftime('%Y%m%d_%H%M%S')
        processing_start = datetime.now()
        
        print(f"üì• –ù–∞—á–∏–Ω–∞–µ–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π (batch_id: {batch_id})...")
        
        message_count = 0
        for message in consumer:
            try:
                # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Kafka
                data = message.value
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—É
                try:
                    date_parsed = datetime.strptime(data['date'], '%Y-%m-%d').date()
                except:
                    # –ï—Å–ª–∏ –¥–∞—Ç–∞ –≤ –¥—Ä—É–≥–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, –ø–æ–ø—Ä–æ–±—É–µ–º parseDateTimeBestEffort
                    date_parsed = datetime.now().date()
                
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏
                msg_timestamp = datetime.fromtimestamp(message.timestamp / 1000) if message.timestamp else datetime.now()
                processing_lag = int((datetime.now() - msg_timestamp).total_seconds())
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å –¥–ª—è ClickHouse
                record = [
                    date_parsed,     # date
                    data.get('location_key', 'UNKNOWN'),
                    data.get('new_confirmed', 0),
                    data.get('new_deceased', 0),
                    data.get('new_recovered', 0),
                    data.get('new_tested', 0),
                    batch_id,
                    message.partition,
                    message.offset,
                    datetime.now(), # processing_time
                    f'manual-clickhouse-consumer-{topic}' # consumer_group_id
                ]
                
                batch.append(record)
                message_count += 1
                
                # –ï—Å–ª–∏ —Å–æ–±—Ä–∞–ª–∏ –Ω—É–∂–Ω—ã–π –±–∞—Ç—á - –≤—Å—Ç–∞–≤–ª—è–µ–º –≤ ClickHouse
                if len(batch) >= batch_size:
                    break
                    
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
                continue
        
        # –í—Å—Ç–∞–≤–ª—è–µ–º —Å–æ–±—Ä–∞–Ω–Ω—ã–π –±–∞—Ç—á –≤ ClickHouse
        if batch:
            column_names = [
                'date', 'location_key',
                'new_confirmed', 'new_deceased', 'new_recovered',
                'new_tested', 'batch_id', 'kafka_partition',
                'kafka_offset', 'processing_time', 'consumer_group_id'
            ]
            
            ch_client.insert(
                'raw.covid_manual_consumption',
                batch,
                column_names=column_names
            )
            
            processing_time = (datetime.now() - processing_start).total_seconds()
            
            print(f"‚úÖ –í—Å—Ç–∞–≤–ª–µ–Ω–æ {len(batch)} –∑–∞–ø–∏—Å–µ–π –≤ raw.covid_manual_consumption")
            print(f"   –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time:.2f}s")
            print(f"   Throughput: {len(batch)/processing_time:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫")
        else:
            print("‚ÑπÔ∏è –ù–µ—Ç –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
        consumer.close()
        ch_client.close()
        
        return {
            'messages_processed': message_count,
            'records_inserted': len(batch),
            'batch_id': batch_id,
            'processing_time_seconds': processing_time,
            'topic': topic,
            'kafka_broker': kafka_broker
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ä—É—á–Ω–æ–º –∫–æ–Ω—Å—å—é–º–µ—Ä–µ: {e}")
        raise

def get_consumption_statistics(**context):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä—É—á–Ω–æ–≥–æ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å Kafka Engine"""
    try:
        import clickhouse_connect
        
        print("üìä –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä—É—á–Ω–æ–≥–æ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è...")
        
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse
        ch_config = {
            'host': os.getenv('CLICKHOUSE_HOST', 'localhost'),
            'port': int(os.getenv('CLICKHOUSE_PORT', 8123)),
            'username': os.getenv('CH_USER'),
            'password': os.getenv('CH_PASSWORD'),
            'secure': False
        }
        
        client = clickhouse_connect.get_client(**ch_config)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä—É—á–Ω–æ–≥–æ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è
        manual_stats_sql = """
        SELECT 
            'Manual Consumer' as method,
            count() as total_records,
            uniq(location_key) as unique_locations,
            min(date) as earliest_date,
            max(date) as latest_date,
            max(processing_time) as last_insertion,
            uniq(batch_id) as total_batches
        FROM raw.covid_manual_consumption
        WHERE processing_time >= today()
        """
        
        manual_result = client.query(manual_stats_sql)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Kafka Engine –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        kafka_engine_stats_sql = """
        SELECT 
            'Kafka Engine' as method,
            count() as total_records,
            uniq(location_key) as unique_locations,
            min(date) as earliest_date,
            max(date) as latest_date,
            0 as avg_lag_seconds,
            max(date) as last_insertion,
            0 as total_batches
        FROM raw.covid_new_cases
        """
        
        kafka_result = client.query(kafka_engine_stats_sql)
        
        print("üìã –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö:")
        
        if manual_result.result_rows:
            row = manual_result.result_rows[0]
            print(f"   Manual Consumer: {row[1]} –∑–∞–ø–∏—Å–µ–π, {row[2]} —Å—Ç—Ä–∞–Ω, {row[6]} –±–∞—Ç—á–µ–π")
        
        if kafka_result.result_rows:
            row = kafka_result.result_rows[0]
            print(f"   Kafka Engine: {row[1]} –∑–∞–ø–∏—Å–µ–π, {row[2]} —Å—Ç—Ä–∞–Ω")
        
        client.close()
        
        return {
            'manual_consumer_stats': manual_result.result_rows[0] if manual_result.result_rows else None,
            'kafka_engine_stats': kafka_result.result_rows[0] if kafka_result.result_rows else None
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        raise

# –°–æ–∑–¥–∞–Ω–∏–µ DAG
dag = DAG(
    'covid_manual_kafka_consumer',
    default_args=default_args,
    description='–†—É—á–Ω–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö Kafka –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Kafka Engine (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥)',
    schedule='*/10 * * * *',  # –ó–∞–ø—É—Å–∫ –∫–∞–∂–¥—ã–µ 10 –º–∏–Ω—É—Ç –≤ 00 —Å–µ–∫—É–Ω–¥
    catchup=False,
    tags=['covid19', 'kafka', 'manual-consumer', 'hw17', 'alternative'],
)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á DAG
setup_table_task = PythonOperator(
    task_id='setup_manual_consumer_table',
    python_callable=setup_manual_consumer_table,
    dag=dag,
)

consume_batch_task = PythonOperator(
    task_id='consume_kafka_batch',
    python_callable=consume_kafka_messages_batch,
    dag=dag,
)

get_stats_task = PythonOperator(
    task_id='get_consumption_statistics',
    python_callable=get_consumption_statistics,
    dag=dag,
)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
setup_table_task >> consume_batch_task >> get_stats_task