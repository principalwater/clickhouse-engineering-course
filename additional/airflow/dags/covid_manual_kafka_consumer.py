from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
import json
import os
import logging
from dotenv import load_dotenv

# Load environment variables
dotenv_path = os.path.join(os.path.dirname(__file__), '../../infra/env/clickhouse.env')
load_dotenv(dotenv_path=dotenv_path)

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ DAG
default_args = {
    'owner': 'hw17-manual-consumer',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
    'max_active_runs': 1,  # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ¸Ğ½ Ğ·Ğ°Ğ¿ÑƒÑĞº Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾
}

def setup_manual_consumer_table(**context):
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ´Ğ»Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Kafka Engine"""
    try:
        import clickhouse_connect
        
        print("ğŸ”§ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ´Ğ»Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…...")
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ClickHouse Ğ¸Ğ· Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ
        ch_config = {
            'host': os.getenv('CLICKHOUSE_HOST', 'localhost'),
            'port': int(os.getenv('CLICKHOUSE_PORT', 8123)),
            'username': os.getenv('CH_USER'),
            'password': os.getenv('CH_PASSWORD'),
            'secure': False
        }
        
        # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº ClickHouse
        client = clickhouse_connect.get_client(**ch_config)
        
        # SQL Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ±ĞµĞ· Kafka Engine
        create_sql = """
        CREATE TABLE IF NOT EXISTS raw.covid_manual_consumption ON CLUSTER dwh_test (
            -- ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
            date Date,
            location_key LowCardinality(String),
            new_confirmed Int32,
            new_deceased Int32,
            new_recovered Int32,
            new_tested Int32,
            
            -- ĞœĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ
            batch_id String,
            kafka_partition Int32,
            kafka_offset Int64,
            processing_time DateTime DEFAULT now(),
            consumer_group_id String
        ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/raw/covid_manual_consumption/{uuid}', '{replica}')
        PARTITION BY toYYYYMM(date)
        ORDER BY (date, location_key)
        SETTINGS index_granularity = 8192;
        """
        
        client.command(create_sql)
        print("âœ… Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° raw.covid_manual_consumption ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾")
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹
        distributed_sql = """
        CREATE TABLE IF NOT EXISTS raw.covid_manual_consumption_dist ON CLUSTER dwh_test AS raw.covid_manual_consumption
        ENGINE = Distributed('dwh_test', 'raw', 'covid_manual_consumption', xxHash64(location_key));
        """
        
        try:
            client.command(distributed_sql)
            print("âœ… Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° covid_manual_consumption_dist ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ°")
        except Exception as e:
            print(f"âš ï¸ Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° Ğ½Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° (Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾, ĞºĞ»Ğ°ÑÑ‚ĞµÑ€ Ğ½Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½): {e}")
        
        client.close()
        
        return {
            'table_created': True,
            'target_table': 'raw.covid_manual_consumption',
            'distributed_table': 'raw.covid_manual_consumption_dist'
        }
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹: {e}")
        raise

def consume_kafka_messages_batch(**context):
    """
    ĞŸĞ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ñ‚Ñ‡Ğ° ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Kafka Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Kafka Engine
    
    Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Kafka Engine ClickHouse:
    - ĞŸÑ€ÑĞ¼Ğ¾Ğµ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº Kafka Ñ‡ĞµÑ€ĞµĞ· kafka-python
    - Ğ ÑƒÑ‡Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ offset'Ğ¾Ğ²  
    - Ğ‘Ğ°Ñ‚Ñ‡ĞµĞ²Ğ°Ñ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ClickHouse
    - Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
    """
    try:
        from kafka import KafkaConsumer
        import clickhouse_connect
        
        processing_time = 0
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ
        batch_size = context.get('params', {}).get('batch_size', 50)
        timeout_seconds = context.get('params', {}).get('timeout_seconds', 60)
        topic = context.get('params', {}).get('topic', 'covid_new_cases_1min')
        
        print(f"ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ÑÑŒÑĞ¼ĞµÑ€Ğ° Kafka")
        print(f"   Topic: {topic}")
        print(f"   Batch size: {batch_size}")
        print(f"   Timeout: {timeout_seconds}s")
        
        # ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Kafka
        kafka_broker = os.getenv('KAFKA_BROKER', 'kafka:9092')
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Kafka ĞºĞ¾Ğ½ÑÑŒÑĞ¼ĞµÑ€
        consumer = KafkaConsumer(
            topic,
            bootstrap_servers=[kafka_broker],
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            group_id=f'manual-clickhouse-consumer-{topic}',
            auto_offset_reset='latest',
            consumer_timeout_ms=timeout_seconds * 1000,
            enable_auto_commit=True
        )
        
        print(f"âœ… Kafka ĞºĞ¾Ğ½ÑÑŒÑĞ¼ĞµÑ€ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½ Ğº {kafka_broker}")
        
        # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº ClickHouse
        ch_config = {
            'host': os.getenv('CLICKHOUSE_HOST', 'localhost'),
            'port': int(os.getenv('CLICKHOUSE_PORT', 8123)),
            'username': os.getenv('CH_USER'),
            'password': os.getenv('CH_PASSWORD'),
            'secure': False
        }
        
        ch_client = clickhouse_connect.get_client(**ch_config)
        print("âœ… ClickHouse ĞºĞ»Ğ¸ĞµĞ½Ñ‚ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½")
        
        # Ğ¡Ğ±Ğ¾Ñ€ Ğ±Ğ°Ñ‚Ñ‡Ğ° ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹
        batch = []
        batch_id = datetime.now().strftime('%Y%m%d_%H%M%S')
        processing_start = datetime.now()
        
        print(f"ğŸ“¥ ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ (batch_id: {batch_id})...")
        
        message_count = 0
        for message in consumer:
            try:
                # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Kafka
                data = message.value
                
                # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµĞ¼ Ğ´Ğ°Ñ‚Ñƒ
                try:
                    date_parsed = datetime.strptime(data['date'], '%Y-%m-%d').date()
                except:
                    # Ğ•ÑĞ»Ğ¸ Ğ´Ğ°Ñ‚Ğ° Ğ² Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ, Ğ¿Ğ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ parseDateTimeBestEffort
                    date_parsed = datetime.now().date()
                
                # Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
                msg_timestamp = datetime.fromtimestamp(message.timestamp / 1000) if message.timestamp else datetime.now()
                processing_lag = int((datetime.now() - msg_timestamp).total_seconds())
                
                # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ğ´Ğ»Ñ ClickHouse
                record = [
                    date_parsed,     # date
                    data.get('location_key', 'UNKNOWN'),
                    data.get('new_confirmed', 0),
                    data.get('new_deceased', 0),
                    data.get('new_recovered', 0),
                    data.get('new_tested', 0),
                    batch_id,
                    message.partition,
                    message.offset,
                    datetime.now(), # processing_time
                    f'manual-clickhouse-consumer-{topic}' # consumer_group_id
                ]
                
                batch.append(record)
                message_count += 1
                
                # Ğ•ÑĞ»Ğ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğ¹ Ğ±Ğ°Ñ‚Ñ‡ - Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² ClickHouse
                if len(batch) >= batch_size:
                    break
                    
            except Exception as e:
                print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ: {e}")
                continue
        
        # Ğ’ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±Ğ°Ñ‚Ñ‡ Ğ² ClickHouse
        if batch:
            column_names = [
                'date', 'location_key',
                'new_confirmed', 'new_deceased', 'new_recovered',
                'new_tested', 'batch_id', 'kafka_partition',
                'kafka_offset', 'processing_time', 'consumer_group_id'
            ]
            
            ch_client.insert(
                'raw.covid_manual_consumption',
                batch,
                column_names=column_names
            )
            
            processing_time = (datetime.now() - processing_start).total_seconds()
            
            print(f"âœ… Ğ’ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ {len(batch)} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ² raw.covid_manual_consumption")
            print(f"   Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: {processing_time:.2f}s")
            print(f"   Throughput: {len(batch)/processing_time:.1f} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹/ÑĞµĞº")
        else:
            print("â„¹ï¸ ĞĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸")
        
        # Ğ—Ğ°ĞºÑ€Ñ‹Ğ²Ğ°ĞµĞ¼ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ
        consumer.close()
        ch_client.close()
        
        return {
            'messages_processed': message_count,
            'records_inserted': len(batch),
            'batch_id': batch_id,
            'processing_time_seconds': processing_time,
            'topic': topic,
            'kafka_broker': kafka_broker
        }
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ² Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½ÑÑŒÑĞ¼ĞµÑ€Ğµ: {e}")
        raise

def get_consumption_statistics(**context):
    """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Kafka Engine"""
    try:
        import clickhouse_connect
        
        print("ğŸ“Š ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ...")
        
        # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº ClickHouse
        ch_config = {
            'host': os.getenv('CLICKHOUSE_HOST', 'localhost'),
            'port': int(os.getenv('CLICKHOUSE_PORT', 8123)),
            'username': os.getenv('CH_USER'),
            'password': os.getenv('CH_PASSWORD'),
            'secure': False
        }
        
        client = clickhouse_connect.get_client(**ch_config)
        
        # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ
        manual_stats_sql = """
        SELECT 
            'Manual Consumer' as method,
            count() as total_records,
            uniq(location_key) as unique_locations,
            min(date) as earliest_date,
            max(date) as latest_date,
            max(processing_time) as last_insertion,
            uniq(batch_id) as total_batches
        FROM raw.covid_manual_consumption
        WHERE processing_time >= today()
        """
        
        manual_result = client.query(manual_stats_sql)
        
        # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Kafka Engine Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ
        kafka_engine_stats_sql = """
        SELECT 
            'Kafka Engine' as method,
            count() as total_records,
            uniq(location_key) as unique_locations,
            min(date) as earliest_date,
            max(date) as latest_date,
            0 as avg_lag_seconds,
            max(date) as last_insertion,
            0 as total_batches
        FROM raw.covid_new_cases
        """
        
        kafka_result = client.query(kafka_engine_stats_sql)
        
        print("ğŸ“‹ Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…:")
        
        if manual_result.result_rows:
            row = manual_result.result_rows[0]
            print(f"   Manual Consumer: {row[1]} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹, {row[2]} ÑÑ‚Ñ€Ğ°Ğ½, {row[6]} Ğ±Ğ°Ñ‚Ñ‡ĞµĞ¹")
        
        if kafka_result.result_rows:
            row = kafka_result.result_rows[0]
            print(f"   Kafka Engine: {row[1]} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹, {row[2]} ÑÑ‚Ñ€Ğ°Ğ½")
        
        client.close()
        
        return {
            'manual_consumer_stats': manual_result.result_rows[0] if manual_result.result_rows else None,
            'kafka_engine_stats': kafka_result.result_rows[0] if kafka_result.result_rows else None
        }
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸: {e}")
        raise

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ DAG
dag = DAG(
    'covid_manual_kafka_consumer',
    default_args=default_args,
    description='Ğ ÑƒÑ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Kafka Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Kafka Engine (Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´)',
    schedule='*/10 * * * *',  # Ğ—Ğ°Ğ¿ÑƒÑĞº ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 10 Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ğ² 00 ÑĞµĞºÑƒĞ½Ğ´
    catchup=False,
    tags=['covid19', 'kafka', 'manual-consumer', 'hw17', 'alternative'],
)

# ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ DAG
setup_table_task = PythonOperator(
    task_id='setup_manual_consumer_table',
    python_callable=setup_manual_consumer_table,
    dag=dag,
)

consume_batch_task = PythonOperator(
    task_id='consume_kafka_batch',
    python_callable=consume_kafka_messages_batch,
    dag=dag,
)

get_stats_task = PythonOperator(
    task_id='get_consumption_statistics',
    python_callable=get_consumption_statistics,
    dag=dag,
)

# ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹
setup_table_task >> consume_batch_task >> get_stats_task