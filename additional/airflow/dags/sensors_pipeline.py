import sys
import os
sys.path.append('/opt/airflow')

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from utils.sensors_producer import SensorsDataProducer

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ Ð´Ð»Ñ DAG
default_args = {
    'owner': 'principalwater',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=1),
    'max_active_runs': 1,
}

def produce_sensors_data(**context):
    """
    ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÑ‚ Ð±Ð°Ñ‚Ñ‡ Ð´Ð°Ð½Ð½Ñ‹Ñ… Environmental Sensors Ð² ÐµÐ´Ð¸Ð½Ñ‹Ð¹ Kafka Ñ‚Ð¾Ð¿Ð¸Ðº,
    Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ "Ð²Ð¾Ð´ÑÐ½Ð¾Ð¹ Ð·Ð½Ð°Ðº" (watermark) Ð´Ð»Ñ Ð¸Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸.
    """
    try:
        params = context.get('params', {})
        broker_url = params.get('broker_url', 'kafka:9092')
        batch_size = params.get('batch_size', 1000)
        topic = params.get('topic', 'sensors')
        use_real_data = params.get('use_real_data', False)
        sensor_types_filter = params.get('sensor_types_filter', context['dag'].params.get('sensor_types_filter'))
        locations_filter = params.get('locations_filter', context['dag'].params.get('locations_filter'))

        last_processed_timestamp = None
        last_timestamp_variable_name = f"{context['dag'].dag_id}_last_timestamp"
        if use_real_data:
            manual_watermark = params.get('initial_watermark_timestamp')
            if manual_watermark:
                print(f"   ðŸ’§ Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ñ€ÑƒÑ‡Ð½Ð¾Ð¹ Watermark, Ð·Ð°Ð´Ð°Ð½Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¸ Ð·Ð°Ð¿ÑƒÑÐºÐµ: {manual_watermark}")
                last_processed_timestamp = manual_watermark
            else:
                last_processed_timestamp = Variable.get(last_timestamp_variable_name, default_var=None)
        
        print(f"ðŸš€ Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ñ€Ð¾Ð´ÑŒÑŽÑÐµÑ€Ð° Environmental Sensors")
        print(f"   Broker: {broker_url}")
        print(f"   Topic: {topic}")
        print(f"   Batch size: {batch_size}")
        print(f"   Real data: {use_real_data}")
        print(f"   Sensor types filter: {sensor_types_filter}")
        print(f"   Locations filter: {locations_filter}")
        if use_real_data and last_processed_timestamp:
            print(f"   ðŸ’§ Watermark (Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¿Ð¾ÑÐ»Ðµ timestamp): {last_processed_timestamp}")
        
        producer = SensorsDataProducer(
            broker_url=broker_url,
            use_real_data=use_real_data,
            data_limit=batch_size * 2, # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ñ Ð·Ð°Ð¿Ð°ÑÐ¾Ð¼
            start_timestamp=last_processed_timestamp,
            sensor_types_filter=sensor_types_filter if use_real_data else None,
            locations_filter=locations_filter if use_real_data else None
        )
        
        sent_count, max_timestamp_in_batch = producer.send_data_batch(
            topic=topic,
            batch_size=batch_size
        )
        
        stats = producer.get_stats()
        producer.close()
        
        print(f"âœ… Ð£ÑÐ¿ÐµÑˆÐ½Ð¾ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¾ {sent_count} ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð² Ñ‚Ð¾Ð¿Ð¸Ðº {topic}")
        
        if use_real_data and max_timestamp_in_batch and max_timestamp_in_batch != last_processed_timestamp:
            print(f"   ðŸ’§ ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Watermark Ð½Ð° Ð½Ð¾Ð²Ñ‹Ð¹ timestamp: {max_timestamp_in_batch}")
            Variable.set(last_timestamp_variable_name, max_timestamp_in_batch)
        elif use_real_data:
            print(f"   ðŸ’§ Watermark Ð½Ðµ Ð¸Ð·Ð¼ÐµÐ½Ð¸Ð»ÑÑ ({last_processed_timestamp}). ÐÐ¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½ÐµÑ‚.")

        return {
            'sent_count': sent_count,
            'topic': topic,
            'batch_size': batch_size,
            'timestamp': datetime.now().isoformat(),
            'producer_stats': stats,
            'last_processed_timestamp': last_processed_timestamp,
            'new_watermark': max_timestamp_in_batch
        }
        
    except Exception as e:
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð² Ð¿Ñ€Ð¾Ð´ÑŒÑŽÑÐµÑ€Ðµ Environmental Sensors: {e}")
        raise

def show_statistics(**context):
    """Ð’Ñ‹Ð²Ð¾Ð´Ð¸Ñ‚ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð¿Ð¾ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸ÑŽ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð¿Ñ€Ð¾Ð´ÑŒÑŽÑÐµÑ€Ð°."""
    ti = context['ti']
    producer_stats = ti.xcom_pull(task_ids='produce_sensors_data')
    
    if producer_stats:
        print("ðŸ“Š Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾Ð´ÑŒÑŽÑÐµÑ€Ð°:")
        print(f"   - ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹: {producer_stats.get('sent_count')}")
        print(f"   - Ð¢Ð¾Ð¿Ð¸Ðº: {producer_stats.get('topic')}")
        print(f"   - Ð Ð°Ð·Ð¼ÐµÑ€ Ð±Ð°Ñ‚Ñ‡Ð°: {producer_stats.get('batch_size')}")
        print(f"   - Ð’Ñ€ÐµÐ¼Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ: {producer_stats.get('timestamp')}")
        print(f"   - ÐŸÑ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ð¹ Watermark: {producer_stats.get('last_processed_timestamp')}")
        print(f"   - ÐÐ¾Ð²Ñ‹Ð¹ Watermark: {producer_stats.get('new_watermark')}")
        
        stats = producer_stats.get('producer_stats', {})
        print("   - Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½ÐµÐ³Ð¾ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ð¿Ñ€Ð¾Ð´ÑŒÑŽÑÐµÑ€Ð°:")
        print(f"     - Ð’ÑÐµÐ³Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð² Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐµ: {stats.get('total_records')}")
        print(f"     - Ð¢ÐµÐºÑƒÑ‰Ð¸Ð¹ Ð¸Ð½Ð´ÐµÐºÑ: {stats.get('current_index')}")
        print(f"     - ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¾ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ñ…ÐµÑˆÐµÐ¹: {stats.get('sent_hashes')}")
        print(f"     - Redis Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½: {stats.get('redis_connected')}")
    else:
        print("   âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð¾Ñ‚ Ð¿Ñ€Ð¾Ð´ÑŒÑŽÑÐµÑ€Ð°.")

# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ DAG
dag = DAG(
    'sensors_pipeline',
    default_args=default_args,
    description='Ð•Ð´Ð¸Ð½Ñ‹Ð¹ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ð´Ð»Ñ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Environmental Sensors Ð² Kafka',
    schedule="* * * * *",  # ÐºÐ°Ð¶Ð´ÑƒÑŽ Ð¼Ð¸Ð½ÑƒÑ‚Ñƒ Ð² :00 ÑÐµÐºÑƒÐ½Ð´
    catchup=False,
    max_active_runs=1,
    tags=['environmental-sensors', 'kafka', 'producer', 'hw18'],
    params={
        'broker_url': 'kafka:9092',
        'topic': 'sensors', 
        'batch_size': 1000,
        'use_real_data': True,
        'sensor_types_filter': [],
        'locations_filter': [],
        'initial_watermark_timestamp': None,
        'max_files_per_run': 10,
    },
)

produce_task = PythonOperator(
    task_id='produce_sensors_data',
    python_callable=produce_sensors_data,
    dag=dag,
)

show_statistics_task = PythonOperator(
    task_id='show_statistics',
    python_callable=show_statistics,
    dag=dag,
)

produce_task >> show_statistics_task
